[["index.html", "Branches of Economics 1 Economics", " Branches of Economics Dyrehaugen Web Notebook 2023-12-07 1 Economics Economics is a broad issue. Here we look at the various Branches of Economic Thought. Economics is too important to leave to the economists "],["ergodicity.html", "2 Ergodicity 2.1 Almost surely 2.2 Cooperation 2.3 Kelly Criterion 2.4 Diversification", " 2 Ergodicity Almost everyone responded to my question about the rationality of expected utility by talking about rationality and utility. But it’s the “expected” part that is the problem. Why would I only care about the mean? (Russel Roberts (Tweet)) EE doesn’t necessarily reject EUT; it offers one interpretation of EUT which is particularly useful and makes strong prediction. You can think of EE as an axiomatization of 19th-century EUT, an alternative axiomatization to von Neumann’s. 2.1 Almost surely Over the very long-term, an individual will tend to get around half heads and half tails. As the number of flips goes to infinite, the proportion of heads or tails “almost surely” converges to 0.5. This means that each person will tend to get a 50% increase half the time (or 1.5 times the initial wealth), and a 40% decrease half the time (60% of the initial wealth). A bit of maths and the time average growth in wealth for an individual is (1.5*0.6)0.5 ~ 0.95, or approximately a 5% decline in wealth each period. Every individual’s wealth will tend to decay at that rate. To get an intuition for this, a long run of equal numbers of heads and tails is equivalent to flipping a head and a tail every two periods. Suppose that is exactly what you did – flipped a heads and then flipped a tail. Your wealth would increase to $150 in the first round ($1001.5), and then decline to $90 in the second ($1500.6). You get the same result if you change the order. Effectively, you are losing 10% (or getting only 1.5*0.6=0.9) of your money every two periods. A system where the time average converges to the ensemble average (our population mean) is known as an ergodic system. The system of gambles above is non-ergodic as the time average and the ensemble average diverge. And given we cannot individually experience the ensemble average, we should not be misled by it. The focus on ensemble averages, as is typically done in economics, can be misleading if the system is non-ergodic. While the population as an aggregate experiences outcomes reflecting the positive expected value of the bet, the typical person does not. The increase in wealth across the aggregate population is only due to the extreme wealth of a few lucky people. 2.2 Cooperation Peters This innocuous-looking gamble is a powerful tool, a window affording us a rather different view of economics, ecology, evolution, and complexity science. (Simulator in post) This coin toss is taunting us — it has that wonderful expected value, increasing exponentially, and yet when we play it, we’re bound to lose. Isn’t there some trick we can apply? Some way of harvesting something of those great expectations, carrying over the promise from the statistical ensemble into the individual trajectory? The answer is yes — and that’s one reason why ergodicity economics has become such a hot topic. There’s a very simple cooperation protocol which allows us to benefit from the coin toss. Here it is: find a partner, independently play one round each, then pool your wealth and split it evenly. Then play the next round independently. With the parameters of the gamble, pairing up in this way leads to a time-average growth rate of the wealth of the cooperating pair of -0.2% per round, compared to -5% per round for the individual player — the cooperators outperform the non-cooperators exponentially and almost break even. With one extra cooperator applying the same protocol — play independently, pool wealth, share equally — the gambling gang moves into positive territory. The time-average growth rate of the cooperating triumvirate is +1.5% per round. In this simple game, Gibran is very literally — mathematically — right: the solitary entity decays, dies with certainty. A few entities who have learned to give, on the other hand, may live. The cooperating gamblers are not doing anything new, in a sense. They’re still just gambling, they haven’t developed any special skills, they can’t predict how the coin will land. All they’ve learned is to share, and just that allows them exponentially to outperform their non-cooperating peers (or former selves). We can keep growing the group, and in the limit of infinitely many cooperators, wealth grows at the growth rate of the expected value. By focusing on expected value, mainstream economics focuses on an object which grows as fast as the wealth of an infinite cooperative. The most skilled can still do better by joining a less skilled collective, and we see mathematically how important it is to maintain diversity and avoid loss of identity in a cooperating group. Athena Aktipis and her coworkers have studied attitudes and moral codes concerning cooperation in different societies, for instance among Maasai pastoralists in East Africa. Their work indicates that where survival is key, more generous systems of mutual aid emerge. Generosity may be thought of as a spectrum reaching from the individual coin toss, where no aid is ever received or provided, to cooperative coin tossing where “aid” is provided at every step whether it’s needed or not. In between lie different forms, where records may be kept to ensure future repayment of aid, or where the severity of need determines the degree of aid provided, with no expectation of repayment. The coin toss says: where unintended consequences can be avoided, the more sharing takes place the faster we will make progress. The optimal level of cooperation appears not as the minimum required to avoid disaster. It is instead the maximum we can get away with without triggering unintended consequences. Peters (2023) For to withhold is to perish Markuu Kurti - comment to Peters Diversification is a negative price lunch April 1, 2023 We will see how the diversification assessment framework provided by conventional finance theory is not applicable to what long-term investors really care about – compounded returns. As long-term investors care about geometric (instead of arithmetic) expected return, we will find that diversifiable risk is not only uncompensated but costly. As a consequence, diversification is not only free, but negative price lunch. What are the implications of all this? Let’s have a look. Arithmetic single period vs. Geometric compounded returns Kurti (2023) Diversification is a negative price lunch 2.3 Kelly Criterion The only way for someone to maintain their wealth would be to bet a smaller portion of their wealth, or to diversify their wealth across multiple bets. The Kelly criterion gives the bet size that would maximise the geometric growth rate in wealth. \\[f = \\frac{bp-q}{b} = \\frac{p(b+1)-1}{b}\\] f is the fraction of the current bankroll to wager b is the net odds received on the wager (i.e. you receive $b back on top of the $1 wagered for the bet) p is the probability of winning q is the probability of losing (1-p) The Kelly criterion is effectively maximising the expected log utility of the bet through setting the size of the bet. The Kelly criterion will result in someone wanting to take a share of any bet with positive expected value. An alternative more general formula for the Kelly criterion that can be used for investment decisions is: \\[f = \\frac{p}{a} - \\frac{q}{b}\\] f is the fraction of the current bankroll to invest b is the value by which your investment increases (i.e. you receive $b back on top of each $1 you invested) a is the value by which your investment decreases if you lose (the first formula above assumes a=1) p is the probability of winning q is the probability of losing (1-p) (More on Evolving Preferences) A Primer (Jason Collins) 2.4 Diversification Kurtti The diversification assessment framework provided by conventional finance theory is not applicable to what long-term investors really care about – compounded returns. As long-term investors care about geometric (instead of arithmetic) expected return, diversifiable risk is not only uncompensated but costly. As a consequence, diversification is not only free, but negative price lunch. But there is a cost: if you want to enjoy the benefits of compounding, you need to pay volatility tax (also called variance drag). Variance drag means that your average annualized geometric (compounded) return is lower than your average annualized arithmetic (single period) return. Annualized volatilities of these two types of returns are approximately equal (when measured from returns with short enough time interval; like daily or monthly returns). With compounding over time (e.g. a stock portfolio with dividends reinvested), annualized return distribution converges towards geometric expected return. Compounding introduces skewness and will break the symmetry of the return distribution. he effect of variance drag is shown in the figure where the geometric mean excess return of a fully diversified portfolio is lower than arithmetic mean excess return (due to systematic variance drag) and the geometric mean excess returns of less than perfectly diversified portfolios are lower (due to idiosyncratic variance drag) compared to fully diversified portfolio. The figure vividly demonstrates that idiosyncratic (firm specific, diversifiable) variance is not only uncompensated but costly. The cost is lower expected portfolio growth rate (geometric rate of return). Diversification, by lowering the cost, increases the expected annualized geometric return. Simultaneously (similarly as with arithmetic returns), diversification reduces risk (annualized standard deviation of geometric returns). In other words, diversification is not only free but negative price lunch. Geometric expected return and therefore also geometric risk premium (expected geometric return in excess of riskless rate) decreases as volatility increases. Geometric risk premium is arithmetic risk premium minus two variance drags: 1) systematic (undiversifiable) variance drag and 2) idiosyncratic (diversifiable, firm specific) variance drag. The latter drag explains why diversification, which decreases idiosyncratic variance, increases geometric risk premium. It is worth noting that variance drag scales as a square of investment fraction emphasizing the importance of stock allocation and leverage. Diversification clearly is the more important the higher your stock allocation and especially if you leverage your stock exposure. Note also that all of the formulas assume continuously compounded geometric returns (i.e. logarithmic returns: ln(1 + arithmetic return)). Also, all of the shown empirical results (like geometric risk premiums) use continuously compounded returns. Sometimes you hear that diversification is important because there are some rare super stocks with very high long-term returns and missing those will cause you to lose to benchmark index. It is true that such super stocks exist, but the existence of super stocks is a consequence, not a cause. The mathematical root cause why super stocks exist is idiosyncratic variance. We can select our portfolio once and keep it unchanged (apart from rebalancing) for a long time or we can re-select our portfolio every month to randomize the super stock exposure over time, and the resulting return distributions will be very similar. Idiosyncratic variance causes both the existence of the super stocks and the total failures. By diversifying we mitigate idiosyncratic variance which is to say we mitigate the effect of both (losing and winning) tails of the return distribution. Simultaneously we increase our geometric expected return by mitigating idiosyncratic variance regardless of whether our portfolio includes super stocks or not. The conventional way to estimate forward looking expected geometric returns by extrapolating realized long-term market return without taking diversification premium differences into account is simply wrong. Small/micro-caps may be more inefficiently priced, but the flip side is that they come with much higher idiosyncratic variance i.e. much higher variance drag. There is no free lunch in harvesting returns in less efficiently priced pool of smaller firms as you need more stock picking skill diluting diversification to overcome the idiosyncratic variance disadvantage compared to big stocks. Conventional wisdom has it that diversification does not help when it is the most needed because correlations rise and systematic volatility peaks at crisis. But diversification was never supposed to help mitigate systematic variance. If it did, we would not have risk premium. We diversify to reduce idiosyncratic variance and, based on history, idiosyncratic variance (diversification benefit) is at its highest during severe bear markets. We can decompose geometric risk premium to geometric risk premium of a single stock and diversification premium. Geometric risk premium of a single stock explains market volatility. Unlike geometric risk premium, diversification premium is always positive and, compared to risk premium, very consistent over time. Historical geometric equity risk premium before modern financial technology is a highly theoretical construct which was never practically achievable to investors. (Same applies to size premium). If we deduct the historical idiosyncratic variance drag, trading costs and other frictions from the historical risk premium (or size premium) and account for reduced risk, we end up with a much smaller number. This is one piece to equity risk premium puzzle. Kurtti (2023) Diversification is a negative price lunch "],["austrian.html", "3 Austrian 3.1 Where Austrians got it wrong 3.2 Austrian Fascism 3.3 Roundaboutness", " 3 Austrian MMT and Austrian economics are mirror images. MMT wants soft money to redistribute wealth to middle class. Austrians want hard money to maintain value of middle class savings. Dominant capitals, in control of state, ignore both and switch between soft and hard to maintain power. (Ian Wright) 3.1 Where Austrians got it wrong On Colin Drumm https://twitter.com/drumm_colin/status/1468779685916012546 Thread Reader https://threadreaderapp.com/thread/1441459276506021890.html Ethan Buchman 24 Sep, 24 tweets, 5 min read Allow me to attempt a more nuanced story of where the Austrians actually did go “wrong” and how they wound up so vilified. I’m still learning the history and formulating my thoughts, but here’s a humble attempt: 1/ A good deal of original Austrian thought was incorporated into the mainstream. Though it had Austrian origins, it became no longer “Austrian” in spirit. The ideas were so good, they had to be appropriated. 2/ This included, of course, marginal utility theory itself (from Menger) as well as ideas like opportunity cost. And Bohm-Bawerk’s capital theory of interest rates (inter-temporal coordination) was taken up by Wicksell who in turn had a heavy influence on Keynes 3/ Mises and Hayek were also heavily influenced by Wicksell, expounding what would become known as the Austrian Business Cycle Theory (ABCT), the foundation for their opposition to govt monopoly on money 4/ Roughly speaking, this is where the Austrians went astray - in obsessing over money as a neutral, non-political veil over real exchange and production, they were largely in denial of actual history, important parts of theory, and even the real world itself! 5/ Neglect of history is partially due to the earlier war with the German Historical School, the so-called Methodenstreit. The Austrians were hell bent on a Praxeology/“Methodological Individualism” which afforded little role for historical circumstance in economic analysis 6/ This is probably the largest error they made. I’m sure the Methodenstreit was a good time, but the inability to incorporate the historical/anthropological record greatly weakened their ability to theorize. They began from an assumption that prior man was similar to modern man 7/ But this is manifestly not true. Cognition and social structures change markedly over time, and this has significant impact on economic reality and coordination. Two very interesting related points here: 8/ First, Weber, who was actually a member of the Historical School, had a major impact on defining Methodological Individualism, and had a huge influence on Mises! 9/ Second, Schumpeter, who would perhaps be the greatest economist produced out of the Austrian tradition (him and Mises were students together of Bohm-Bawerk) actually defected towards the Historical School. Arguably this enabled him to break free from … 10/ Austrian constraints and develop a more complete theory and history of economics. Notably, his student Hyman Minsky, would get all the credit post 2008 for theories about the inherent instability of credit money, to the chagrin of Austrians everywhere. 11/ But Minsky’s Financial Instability Hypothesis wasn’t the ABCT. It was grounded in analysis of the actual web of liabilities produced by a financial system. While the Austrians wrote it all off as “bad”, Minsky sat down to analyze it and provide some constructive policy! 12/ Part of the failure of the Austrian imagination was to provide some meaningful alternative to abolishing fractional reserve banking in favour of pure commodity money. There are some inklings of the power of trade credit clearing in Mises’ TOMC, but it seemed to stop there 13/ Of course Hayek would eventually propose denationalized fiat, and a later tradition of Austrian inspired economists like (GeorgeSelgin?) and (lawrencehwhite1?) would bring this home through a theory and history of free banking - a stable fractional reserve based system 14/ Not to say Austrians were “wrong”, but there was an inherent sort of “arrested development” in their ideas. To continue the history, despite their brilliance, the Austrians took a beating in the 30s when Hayek tragically “lost” the debate to Keynes: 15/ After the second war, they turned their attention more to politics. Hayek of course founded an information theoretic approach to econ (prices as decentralized network of computation) which would have enormous influence. But increasingly “Austrian” would be less associated 16/ with the brilliant economic work of earlier years (so much of which was appropriated in the mainstream) and increasingly associated with a kind of “reactionary” and seemingly “far right” political philosophy 17/ In my own estimation this is at least a mischaracterization of Hayek’s agenda, though I can’t help but agree that his work suffered from a kind of arrested development. And the founding of the Mont Pelerin Society did not help much. 18/ The Austrian tradition had something of a boom in the 80s when it was vehemently taken up by Reagan and Thatcher, but in a way that was highly discretionary/selective. The existence of their large governments and their alliance with the banker class betrayed the essence of 19/ the Austrian teachings! It’s no wonder this would give rise to an abominable neoliberalism which I can only imagine caused more than a few of the Austrian greats to roll in their grave. Personally, I can’t help but feel there was a certain naiveté 20/ in their political philosophy, perhaps a result of their dismissal of the historical school and denial of the role of State, which ultimately led to their philosophy becoming a weapon of what were perhaps some of the most destructive “peacetime” regimes the West has seen 21/ Of course the Rothbardians would take the political philosophy much farther, and would lead to something of a “split” within the Austrians, seeing Hayek as somehow lesser for affording a greater role for the state and realizing that money need not be based on real commodities 22/ But obviously Hayek was on the right track, distracted though he may have been by the political climate. Arguably, his line of thought matured quite substantially in both the Free Banking school and in the Bloomington School of the Ostroms. 23/ And this is where many of us (in crypto, say) are picking up today. Recognizing the foundational brilliance of the Austrians but seeking to not make the same neoliberal mistakes by instead following the Austrians to their conclusion in the work of the Ostroms: local commons! 3.2 Austrian Fascism Tooze In 1930 when President Hoover began his last-ditch effort to rebuild the internaitonal order, starting with the London conference on naval arms control, fascist Italy, after Ramsay MacDonald’s Labour government in Britain, was Washington’s favored partner in Europe. When Mussolini’s foreign minister, the charismatic ex-squadistra Dino Grandi, met Hoover in 1931, the president is said to have assured his Italian guest that the vocal minority of antifascists in America should be ignored: “They do not exist for us Americans, and neither should they exist for you.” Mussolini’s regime, in other words, was not per se an alien force, an “other” that was rejected from the existing international order. On the contrary it was understood, especially, in the 1920s as a force of order, offering a new set of solutions to the problem of capitalist governance and one which forward-thinking liberals and conservatives associated themselves. And this opens a further disconcerting historical vista. In his elegant study Globalists Quinn Slobodian showed how the Austrian school of neoliberalism emerged after 1918 from the collapse of the Habsburg Empire. It promised to find a way of encasing the economy so that it would be immune to the unleashed politics of national democracy. It would have huge ramifications decades later because of the influence of the Austrian school on Mont Pelerin and the worldwide market revolution. What Mattei shows us, is that a view of economics as having key role in disciplining both state and society went far beyond the confines of the Austrian tradition. It was a basic element in the vision of (new) liberalism from World War I onwards and its influence extended well beyond the diminished crisis-ridden rump of the Habsburg Empire. What motivated it and formed the bridge to the fascists was its desire to restore control - if necessary by repressive means - over government finances, inflation, the workplace and the labour market. The risk otherwise was not Stalinist communism - that was still a far off threat - but a descent into anarchy of the type they saw being played out in civil war Russia. [Tooze (2022) The centenary of Mussolini’s “March on Rome” and the dilemmas of the liberal expert class. ](https://adamtooze.substack.com/p/chartbook-166-19222022-the-centenary] Archipelago Capitalism Quinn Slobodian’s illuminating discussion in Globalists of the relationship between neoliberal economics and the crisis of Empire. He traces the emergence of the original Austrian branch of neoliberalism and the founding of the Mont Pelerin society to the collapse of the Habsburg Empire in the aftermath of World War I. As he argues, Austrian economists of the 1920s saw the democratic nation-state as a threat to the free flow of resources that had been previously secured by Imperial power. A new political economy was required to encase the economy and insulate it from democratic national sovereignty. As I show in the Foreign Policy piece, it is a logic that can be extended in interesting ways to the way in which offshore finance has found a home in the remnants of the British empire in the Caribbean. In The Code of Capital Katherina Pistor showed us how the English common law functions as one of the key systems worldwide for the encoding of capital. Much of the Caribbean and the wider region including, of course, the United States has inherited the English common law from the original moment of settler colonialism when the region was joined in an interconnected system of plantation slavery and long-range commerce. The world of nation state economic policy, “(t)he New Deal, the European welfare state, decolonization, development and modernization projects in the Third World, and the Bretton Woods system” all of which were centered on nation-state-based and government-driven projects. The offshore world by contrast offered enclaves, special economic zones, havens, relaxed regulations and minimal oversight, flags of convenience, anonymous financial and banking institutions. Tooze (2023) Archipelago Capitalism 3.3 Roundaboutness Thornton Economists understand very little about how technological progress occurs. —Alan Greenspan Before we leave the topic of the problems and blessings of roundaboutness of production and the structure of production, it will be very useful to see a natural, concrete example of it in action. It then will become easier to understand the unnatural cases involving malinvestments and the skyscraper curse. Making production processes more roundabout results in greater production in terms of the quantity produced and a lower cost on a per-unit basis. Entrepreneurs would not want to make production processes more roundabout unless they thought they would create more profits as a result. More roundabout production takes more time, more steps, and a more extensive division of labor. It also uses new technology. Entrepreneurs do make mistakes, of course, but the only systematic errors they make are when they are fooled into rearranging production because of artificially low interest rates and easy credit conditions. When the central bank lowers its target interest rates it also makes credit conditions easier in that banks will make a larger volume of loans, which means they weaken their lending standards in order to facilitate the larger volume of loans. A good example of a very direct production process, in contrast to a more roundabout one, is a farmer who goes to the barn, milks a cow, and then returns to the house and feeds the milk to his family. An example of a more roundabout, although still very direct, production process comes from my childhood. We lived on the edge of a small town. Just beyond our house were fields and barns. Dairy cattle would feed on the grass in the fields. Later they would return to the barns to be milked. The milk would then be transported a short distance — a couple miles — in a small tanker truck to one of three small dairies in my hometown. There the milk would be processed and packaged. Early the next morning a dairy man in a white suit would arrive at our house and place several quart-sized glass bottles of milk in an insulated dairy box outside of our back door and pick up any used bottles we had placed there. If we wanted an ice cream sundae, we had to go to the dairy during retail hours. By the time I graduated from high school the entire system had changed. The small dairy farms had been largely replaced with larger farms. The small four-wheel tanker trucks had been replaced by large eighteen-wheel tankers. An eighteen-wheel tanker truck brought the raw milk from the farms to the dairy factory about thirty miles from our house, and a different eighteen-wheel refrigerated truck brought cartons of milk and ice cream as well as boxes of butter to the supermarket. All three of the small hometown dairies eventually went out of business. They were replaced by much larger, factory-sized dairies many miles from our home. Instead of having the milk bottles delivered directly to our house, we now purchased dairy products at the local supermarket, an institution that was also a relatively new phenomenon. The dairy factory system is a much more roundabout production process. It takes more time. The milk travels a round-trip journey of more than sixty miles instead of the less-than-four-mile journey in the old days. There is a greater amount of capital as well as advanced technology involved and there is also far less labor per unit of milk. The overall cost of milk is lower, and with competition between large dairy wholesalers and supermarkets, so is the price. In order to attain a more roundabout production process there are several requirements. It requires entrepreneurs with a vision of the most profitable action among all possible actions. It requires investment in more capital goods and new technology. Of course, all of this rearranging of production is going to take a great deal of time and even more time for it to be profitable. Therefore, the entrepreneurs need to have access to savings. They need to have either their own savings or someone else’s savings on a long-term basis in order to proceed. Hence there must be more overall savings in an economy in order to achieve more roundabout production and all the benefits it entails. Savers must have lower time preferences and be willing to delay some consumption in the present. Savers will be rewarded with interest income, with which they will be able to make a larger amount of purchases in the future and at lower prices because of the increase in production of goods. The whole process is regulated by the rate of interest, the price system, and the system of profit and loss. This process is sometimes referred to as creating economies of scale. But notice that while there are economies of scale in this example, everything about the production process changed. The most successful approach was not preordained or known in times past. The entire recipe or technology of production has changed. All the capital goods — including the milking machines, the trucks, and the machinery inside the dairies — are different. Notice further that the change in the dairy industry is going to induce changes in other industries, including technology and investment in the mechanical milking machines industry. All of this requires a careful synchronization process, which is obviously beyond the scope of central planning. The process is driven by the rate of interest. So we will now see what happens when the interest rate is misleading and results in an economic bust and, in severe cases, the skyscraper curse. Thornton (2017) Why Understanding “Roundaboutness” Is so Important "],["behavioural-economics.html", "4 Behavioural Economics 4.1 Biased Behaviour", " 4 Behavioural Economics Ole Peters The symbol of behavioral economics is a fly etched into a urinal to reduce spillage. Apparently, no scientific study of the effect exists. It’s becoming hard to avoid the impression that behavioral economics, broadly speaking, is a collection of made-up cocktail-party stories. Derman People are bad at making rational decisions. One of the hottest topics in finance and economics for the past two decades has been Behavioral Economics, a field that originated in the research of Daniel Kahneman and Amos Tversky. Tversky died in 1996, and Kahneman was awarded The Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel in 2002. The Nobel committee cited their joint work on “prospect theory as an alternative, that better accounts for observed behavior” of humans making decisions “when future consequences are uncertain” (aren’t they always?). Classical financial modeling assumes that people make decisions in a cold-blooded utilitarian way that is therefore susceptible to mathematics and statistics. Kahneman and Tversky (K&amp;T) cataloged a collection of irrational warm-blooded peculiarities in the way people choose between alternative bets on their own potential profits and losses when playing games of chance. Not everyone agrees with K&amp;T. Real life is not always a game of chance; while the probability of throwing heads and tails is known exactly, the probability of human behavior is not. Animate individuals are driven by motives that can defy statistics. Our legal system recognizes this, and finds defendants guilty or innocent not on the basis of statistical evidence but on the basis of judgement and believability. Prospect Theory K&amp;T developed what they call prospect theory. In prospect theory, as opposed to classical economic theory, K&amp;T replaced homo economicus’s rational notions of losses and gains and their probabilities by the empirically determined “irrational” values used by everyday fearful and greedy hot-blooded homo affectus. Classical economic theory was elegant but flawed, and prospect theory was a beautiful idea/ideal that aimed to fix it by taking account of actual human preferences in determining economic value. Unfortunately, that isn’t what happened. First, the ambitions of prospect theory as a science of human behavior foundered in a maelstrom of increasing mathematical complexity. Second, academics use the cover of behavioral economics to write papers on all sorts of irrelevant apparent irrationalities. Third, the part of behavioral economics that did flourish enormously is the notion that people are probabilistically challenged, and that it requires governments and agencies, helped by academics, to nudge people into doing what is “good” for them. Consider Richard Thaler, a Chicago academic who has been an influential and early researcher in behavioral finance and is also the co-author of Nudge, a book he says is about “enlisting the science of choice to make life easier for people and by gentling nudging them in directions that will make their lives better.” It’s remarkable that behavioral economics has evolved from a field of study into a tool for manipulating people. This is not government of the people by the people for the people. I think I’d rather be forced than nudged. At least then the battle lines are clearer. Derman (2020) Misbehavioural (pdf) 4.1 Biased Behaviour Fix According to behavioral economics, most human decisions are mired in ‘bias’. It muddles our actions from the mundane to the monumental. Human behavior, it seems, is hopelessly subpar.1 Or is it? You see, the way that behavioral economists define ‘bias’ is rather peculiar. It involves 4 steps: Start with the model of the rational, utility-maximizing individual — a model known to be false; Re-falsify this model by showing that it doesn’t explain human behavior; Keep the model and label the deviant behavior a ‘bias’; Let the list of ‘biases’ grow. Jason Collins (an economist himself) thinks this bias-finding enterprise is weird. In his essay ‘Please, Not Another Bias!’, Collins likens the proliferation of ‘biases’ to the accumulation of epicycles in medieval astronomy. Convinced that the Earth was the center of the universe, pre-Copernican astronomers explained the (seemingly) complex motion of the planets by adding ‘epicycles’ to their orbits — endless circles within circles. Similarly, when economists observe behavior that doesn’t fit their model, they add a ‘bias’ to their list. The accumulation of ‘biases’, Collins argues, is a sign that science is headed down the wrong track. What scientists should do instead is actually explain human behavior. To do that, Collins proposes, you need to start with human evolution. The ‘goal’ of evolution is not to produce rational behavior. Evolution produces behavior that works — behavior that allows organisms to survive. If rationality does evolve, it is a tool to this end. On that front, conscious reasoning appears to be the exception in the animal kingdom. Most animals survive using instinct. An organism’s ‘bias’ should be judged in relation to its evolutionary environment. Otherwise you make silly conclusions — such as that fish have a ‘bias’ for living in water, or humans have a ‘bias’ for breathing air. When behavioral economists conclude that our probability intuition is ‘biased’, they assume that its purpose is to understand the god’s eye view of innate probability — the behavior that emerges after a large number of observations. But that’s not the case. Our intuition, I argue, is designed to predict probability as we observe it … in small samples. Fix (2021) Is Human Probability Intuition Actually ‘Biased’? "],["experimental-economics.html", "5 Experimental Economics", " 5 Experimental Economics Investopedia Experimental economics is used to help understand how and why markets function the way they do. These market experiments, involving real people making real choices, are a way of testing whether theoretical economic models actually describe market behavior, and provide insights into the power of markets and how participants respond to incentives—usually cash. The field was pioneered by Vernon Smith. Experimental economics is mainly concerned with testing in a laboratory setting with appropriate controls to remove the effects of external influences. Participants in an experimental economics study are assigned the roles of buyers and sellers and rewarded with the trading profits they earn during the experiment. The promise of a reward acts as a natural incentive for participants to make rational decisions in their self-interest. During the experiment, researchers constantly modify rules and incentives in order to record participant behavior in changed circumstances. Smith’s early experiments focused on theoretical equilibrium prices and how they compared to real-world equilibrium prices. He found that even though humans suffer from cognitive biases, traditional economics can still make accurate predictions about the behavior of groups of people. Groups with biased behavior and limited information still reach the equilibrium price by becoming smarter through their spontaneous interaction. The applications of experimental economics can be seen in various policy decisions. For example, the design of carbon trading emissions schemes has benefitted from experiments conducted by economists in different regions of the world in a laboratory setting. Investopedia: Experiemental Economics Nobel Nobel (2002) Kahneman and Smith (pdf) "],["heterodox-economics.html", "6 Heterodox Economics", " 6 Heterodox Economics Roberts There are economists who have serious criticisms of mainstream market economics. There is what we can call the heterodox schools of economics – the term meaning what it says, outside the orthodox mainstream. Within this broad strand, these economists highlight the irrational behaviour of markets and the inherent instability of the market economy. They include the Marxist school which argues the market economy will always have crises that cannot be resolved by the market and so the market economy (called capitalism by Marxists) needs to be replaced by a planned economy based on common ownership of all producers. The heterodox school is very critical of the mainstream. Indeed, almost exactly six years ago, leading heterodox economists held a seminar right here at the LSE on the state of mainstream economics, as taught in the universities. They kicked this off by nailing a poster with 33 theses critiquing mainstream economics to the door of this building. (You can google it). It was the 500th anniversary of when Martin Luther nailed his 95 theses to the Castle Church, Wittenberg which provoked the beginning of the Protestant reformation against the ‘one true religion’ of Catholicism. The heterodox economists were telling us that mainstream economics was like Catholicism and must be protested against as Luther did back in 1517. As they put it, “Economics is broken. From climate change to inequality, mainstream (neoclassical) economics has not provided the solutions to the problems we face and yet it is still dominant in government, academia and other economic institutions. It is time for a new economics.” Roberts (2023) Why real-world economics matters "],["institutional.html", "7 Institutional 7.1 Veblen", " 7 Institutional 7.1 Veblen Like his fellow economists, Veblen was excited by the prospect that, after Darwin, the study of human society could be placed on a scientific footing. Unlike most of his colleagues, he did not think the economic system was working. In one way or another, they maintained that an economy metes out its own rewards in proportion to the productivity of those who constitute the economy. For Veblen, it was not the fittest—that is, the most productive—who were surviving and prospering. On the contrary, the winners were a “leisure class” of unproductive parasites devoted to what he called “conspicuous consumption.” In the process, they were damaging rather than serving the interests of society as a whole. This was, to speak in rational terms, inefficient and irrational. Capitalism = Plunder Veblen’s early translation of the Icelandic epic [Laxdøla-saga] did foretell the visionary economics that would carry him to fame. In the introduction, added thirty-five years later, he wrote that “the Viking age was an enterprise in piracy and slave-trade” and that the Vikings thus anticipated modern “business enterprise,” which is driven “by its quest for profits” and reliant on “getting something for nothing by force and fraud.” Behind the gently waggish satire of terms like “leisure class” and “conspicuous consumption” lies a proposition that is much sterner, and more scientific. In Veblen’s view, modern capitalism, which congratulates itself on its high level of civilization, is in essence a highly organized system of barbaric plunder. He was also lucky to take classes from a young professor named John Bates Clark (1847–1938)—“one of the most important economic theorists America has ever produced,” in Camic’s opinion. Though Clark exposed Veblen to the classical economics descended from Adam Smith and embodied by John Stuart Mill, he added his own “strong objections.” Like the so-called “historical” school of economics, which he had encountered during two years of study in Germany, Clark insisted that economic man was not a mere creature of self-interest, but rather a social being. Morality exerted an objectively measurable influence on economic life, and economics had to be cognizant of it. At that stage of his career, Clark identified as a “Christian socialist.” Veblen himself already had a reputation as an atheist. Veblen’s doctorate in 1884, which he received after transferring to Yale, was one of the first dozen granted in philosophy at any American university. What followed, as noted above, was years of unemployment. In all his many writings, Veblen distinguished between the industrial, which is about making useful things and providing useful services, and the pecuniary, which is about trying to get something for nothing. It is the pecuniary, he argued, that has become the dominant mode of the modern American economy and that recalls his predatory Viking ancestors. But his scorn for pecuniary plunderers was also an inheritance. Where he grew up, the value of honest labor and the denunciation of “idleness, waste, extravagant display, and ill-gotten acquisitions” were drilled into the local children in church and in elementary school, and Camic explains why they would be. The family farm, “which both owns the means of production and provides the labor power to set them in motion,” if not exactly outside the capitalist system, was external enough to generate sharp critique of that system, especially as banks, railroads, and middlemen gradually extended their influence. Robbins (2021) A Theory of Thorstein veblen "],["keynesian-economics.html", "8 Keynesian Economics", " 8 Keynesian Economics Keynesian theory rejects the equilibrium idea of the calm pool of the neoclassical school. The Keynesians think the neoclassical model is not ‘real world’ economics. The Keynesians argue that market economies sometimes get into ‘disequilibrium’ leading to depressions and unemployment, which economies do not get out of unless governments intervene with measures including printing more money or increasing government spending to restore equilibrium. twitter thread to fig Mason A central divide between Keynesian and orthodox macroeconomic theory is the view of the interest rate. Mainstream textbooks teach that the interest rate is the price of saving, balancing consumption today against consumption in the future — a tradeoff that would exist even in a nonmonetary economy. Keynes’ great insight was that the interest rate in a monetary economy has nothing to do with saving but is the price of liquidity, and is fundamentally under the control of the central bank. He looked forward to a day when this rate fall to zero, eliminating the income of the “functionless rentier”. Kenesian Climate Economics As applied to climate policy, this view has several implications. First, market interest rates tell us nothing about any tradeoff between current living standards and action to protect the future climate. Second, there is no reason to think that interest rates must, should or will rise in the future; debt-financed climate investment need not be limited on that basis. Third, while investment in general is not very sensitive to interest rates, an environment of low rates does favor longer-term investment. Fourth, low interest rates are the most reliable way to reduce the debt burdens of the public (and private) sector, which is important to the extent that high debt ratios constrain current spending. Mason (2021) Climate Policy from a Keynesian Perspective "],["marxist.html", "9 Marxist 9.1 Social Monopoly", " 9 Marxist Trigg Abstract There is general agreement amongst scholars of Marx that his monetary theory is incomplete, especially in his most detailed writings on credit in the third volume of Capital. Moreover, in these unfinished notes Marx takes sides with the banking school approach, notable for its opacity compared to the clear axioms of its currency school counterpart. A reconstruction is proposed based on Marx’s step-by-step method, commencing with a critique of Say’s Law under simple commodity circulation, these foundations formalised here using the model of pure labour developed by Pasinetti (1993). Piecing together the fragments, and filling in some of the gaps in Marx’s writings on money, the analysis builds from commodity money and private debt contracts, to the modelling of pure credit and pure banking systems. Adapting the Pasinetti model of a real economy, its endogenous money requirements provide an alternative to the exogenous money approach of the currency school: a streamlined analytical core to the banking school approach, as interpreted by Marx. In addition, the structure of payment crises — as an extension of Marx’s possibility theory of crises — is examined with money as a means of payment required to settle debts between producers and the banking system. Trigg (2021) Reconstructing Marx’s Theory of Credit and Payment Crises under Simple Circulation (pdf) 9.1 Social Monopoly Roberts on Stiglitz After all, monopoly power is really oligopoly (a few large companies) and oligopoly can exhibit fierce competition, nationally and internationally. The real cause of inequality is not monopoly but the increased exploitation of labour by big capital since the 1980s in the effort to reverse falling and low profitability experienced in the 1970s. And the real cause of ‘stagnation’ and low productivity growth is not monopoly but the failure to invest, not only by large ‘monopolies’ but also by smaller capitals suffering from low profitability and high debts. In other words, it is not monopoly that is the problem per se, but the weakness of the capitalist mode of production where investment and employment is only for profit. This Stiglitz ignores. As a result, his solution of government intervention to reduce inequality and create a more ‘level playing field’ for ‘competition’ among capitalist companies is utopian (you can’t turn the capitalist clock back) and unworkable (it would not achieve greater equality or better growth) Ironically, there is another study that Stiglitz has not noticed that shows the rise in US inequality has coincided with the decline of large companies that used to employ hundreds of thousands or even millions of workers and their substitution by much smaller companies. The share of large employers in total US employment went down simultaneously with the increase in US income inequality. This study shows that is the decline in the power of labour through out-sourcing and globalisation that has driven up inequality in incomes. The ‘internal’ break-up of large company (Fordist) employment into small contractors is the key feature of Stiglitz’s world of ‘monopoly’. In other words, what workers in America need is not the break-up of monopolies to create small companies in competition but trade unions. The monopoly power that matters is that held by capital over labour. Yes, monopoly (more accurately oligopoly) power has increased in the last 150 years since Marx forecast that the capitalist mode of production would lead to increased concentration and centralisation of capital. And that shows that capitalism is in its late stage of development and so must be replaced by ‘social monopoly’. Roberts (2016) Monopoly or competition: which is worse? "],["neo-classical-economics.html", "10 Neo Classical Economics 10.1 General Equilibrium 10.2 Free Market 10.3 Trade Theory 10.4 Human Capital Theory 10.5 Efficiency 10.6 Socialist Alternative to Human Capital Theory 10.7 Production Function 10.8 Keen Critique", " 10 Neo Classical Economics The development of neoclassical economics can be read as one long apology for why capitalists deserve their income. Fix (2023) Inflation regulation by wage hikes Neoclassicism is a religion dressed as a science Neoclassical economic theory has never resembled a scientific enterprise. It’s simply an ideology presented through an avalanche of mathematics. The underlying assumptions of neoclassical theory all serve to justify the capitalist status quo. When we equate market value with utility, we implicitly assume that individuals’ income indicates their contribution to society. The neoclassical school starts from a basic assumption that a ‘free market’ ie. without interference or imperfections caused by monopolies or trade unions or the government, will deliver harmonious economic improvement in what is called a ‘general equilibrium’. As one neoclassical economist once put it: ‘the market economy is like a calm lake or pool. Sometimes a rock or stone can disturb it, a shock to the calm environment, but eventually if those interferences stop, the ripples in the pool will subside and the pool will be calm again’. 10.1 General Equilibrium Ayres In a closed Walrasian model resources are assumed t o be generated by labor and capital. The neo-classical (Walrasian) equilibrium system does not qualify as a dissipative structure. The neoclassical system is, in effect, a perpetual motion machine. Ayres (1988) Self-organisation in Biology and Economics (pdf) 10.2 Free Market Fix According to neoclassical economics, the most efficient way to organize hu- man activity is to use the free market. By stoking self interest, the theory claims, individuals can benefit society. This idea, however, conflicts with the evolution- ary theory of multilevel selection, which proposes that rather than stoke self interest, successful groups must suppress it. Which theory better describes how human societies develop? I seek to an- swer this question by studying the opposite of the market: namely hierarchy. I find evidence that as human societies develop, they turn increasingly to hier- archical organization. Yet they do so, paradoxically, at the same time that the language of free markets becomes more common, and culture becomes more individualistic. This evidence, I argue, contradicts free-market theory, but only if we treat it as a scientific doctrine. If instead we treat free-market theory as an ideology, the pieces come together. Free-market thinking, I speculate, may stoke the forma- tion of hierarchy by cloaking power in the language of ‘freedom’. In this evolutionary context, the theory of free markets is an outlier. It posits that, contrary to what we observe among other social organisms, humans need not suppress self-interest to organize in large groups. And we need not use hierarchical organization. We can build complex societies, the theory claims, using decentralized competition. Treating firms (notindividuals) as the unit of competition legitimizes the firm as an autonomous unit, while leaving the firm’s internal structure as a ‘black box’. By championing firm autonomy, free-market theory may legitimize the firm’s internal chain of command, thereby justifying the accumulation of power. Neoclassical economics may be best treated as a belief system whose existence should be explained using the tools of cultural evolution. Neoclassical theory — claims that outcomes from perfectly competitive markets are ‘optimal’, whereas outcomes from centralized control are ‘inefficient’. It is much like if biologists deemed single-celled organisms to be ‘optimal’, but deemed multicellular organisms ‘inefficient’. Hierarchi - supressing lower-level selection More complex structure is built from simpler components. Growth of complexity involve the centralization of control. Nested hierarchy occurs through a process of evolutionary problem solving. Structures evolve that solve specific problems. Newly created structure serves as the building block to solve new problems. Large, complex organisms are not composed of autonomous units. The growth of complexity involve gradual loss of autonomy among sub-units and the growth of centralized control. As societies become more populous, they add new layers of administrative hierarchy. Centralized control arise for two (related) reasons. First, assembling a larger system from many smaller components requires coordination. Second, there is the problem of the ‘self-interest’ of sub-units. The major evolutionary transitions happened by merging sub-units that were previously autonomous. According to the theory of multilevel selection, this merger is not possible unless the ‘self-interest’ of sub-units is suppressed. The key insight of multilevel selection theory is that high-level organization requires high-level selection that suppresses selection at lower levels. Group-level selection suppresses individual-level selection. Successful groups suppress lower levels of selection by turning to top-down ‘management’. Large-scale organization is accomplished by integrating subunits into a hierarchical control structure. Whether complex organization requires hierarchy is an open question. But it does seem that complexity and hierarchy go hand in hand. Free Market - No Hierarki According to the neoclassical theory of free markets, hierarchy is unnecessary for group organization. Instead, neoclassical theory argues that humans can organize effectively without any form of centralized control. All that is needed is a competitive market. The ‘first fundamental theorem of welfare economics’claims that under conditions of perfect competition (in which all firms are ‘price takers’), markets will allocate resources in a way that is ‘Pareto efficient’ With their welfare theorem in hand, neoclassical economists look at hierarchical organization and see an ‘inefficient’ system. The growth of hierarchy with economic development Fix (2021) Economic Development and the Death of the Free Market (pdf) 10.3 Trade Theory International trade theory’s prediction of equalization of wages across coun- tries is, in my view, the key terrible simplification that causes world hunger. Not only are all qualitative diff erences assumed away, the production process itself is also abstracted away. Assuming away unemployment, as the World Bank traditionally does in its models, only adds another dimension to the terrible simplifi cation on which our world economic order is based. In many countries, 80 per cent of the potentially active population are unemployed or underemployed. Assuming that fact away is a terrible simplifi cation. At the core of our world economic order lie the terrible simplifi cations of international trade theory. Assuming perfect information (i.e. that all know the same) and constant returns to scale for all ranges of output for all goods (i.e. no fi xed costs), and assuming that all goods are private, there is no reason why there should be any trade at all (except in raw materials, for reasons of climate and geography). In its most simple form, the theory that regulates international trade is based on assumptions that mimic conditions which would not produce any division of labour or any trade. It describes a world in which every human being would be a self-suffi cient microcosm. Th e WTO and our world order are based on theories that are, at their very core, fairly simplistic banalities wrapped in an appearance of ‘science’. Both Mill and Keynes saw that poor countries need an increasing returns sector, i.e. an industrial sector, in order to become wealthy. What unites the failure to understand that a fi nancial crisis was coming and persistent poverty in the Th ird World is an economic theory at a level of abstraction where production is left out, a theory where the world economy is perceived as stock markets and freight terminals. In reality, markets and trade are mere complements of an incredibly complex global system of production. By focusing on stock exchanges and trade, the complexities of world production have essentially been left out of economic theory. Why are there so few middle-income countries? Why do countries tend to cluster in two convergence groups, developed and ‘underdeveloped’? This paper argues that our inability to create middle-income countries is a result of ‘terrible simplifi cations’ resulting from destabilizing stability, as described by Minsky, from ‘theoretical overshooting’ in Hayek’s sense. The policy recommendations resulting from this theoretical overshooting have made the creation of new middle-income countries virtually impossible. A middle-income nation has an increasing returns (industrial) sector which, for a while, is not yet competitive on world markets. Opening to free trade was supposed to even out world incomes. Th e WTO’s fi rst Director-General, Renato Ruggieri, declared that we should unleash ‘the borderless economy’s potential to equalise relations among countries and regions’. Instead, this process ended up killing the incipient industrial sectors in poor countries, lowering real wages. Th e belief that the market, left to itself, guarantees harmony was at the core of the Washington Consensus ideology of the International Monetary Fund (IMF) and the World Bank. Colonialism Involves a technology policy preventing increasing returns activities from being established in the colonies. A poor nation is much better off with a relatively inefficient manufacturing sector than with no manufacturing sector at all. Reinert (2011) The terrible simplifiers (pdf) 10.4 Human Capital Theory The idea of workers as embodied capital Individual capabilities are fundamentally social The sentiment behind eugenics (that some people are far more productive than others) lingers on in mainstream academia. It survives – even thrives – in human capital theory. In the 1950s, economists at the University of Chicago tackled the question of individual income. Why do some people earn more than others? The explanation that these economists settled on was that income resulted from productivity. The claim that income stems from productivity was not new. It dated back to the 19th-century work of John Bates Clark (1899) and Philip Wicksteed (1894), founders of the neoclassical theory of marginal productivity. Clark and Wicksteed, though, were concerned only with the income of social classes. What the Chicago-school economists did was expand productivist theory to individuals. Doing so required inventing a new form of capital. The idea was that individuals’ skills and abilities actually constituted a stock of capital – human capital. This stock made individuals more productive, and hence, earn more income. The idea that skills constituted “human capital” was initially greeted with skepticism. For one thing, the term itself smacked of slavery. (Capital is property, so “human capital” implies human property.) For another, human capital theory overtly justified inequality. It implied that no matter how fat their incomes, the rich always earned what they produced. Any attempt (by the government) to redistribute income would therefore “distort” the natural order. During the 1950s and 1960s, there was little tolerance for such views. It was the era of welfare-state expansion, driven by Keynesian-style thinking. Yes, big government may have been “distorting” the free market – but society seemed all the better for it. Until the 1970s, human capital theory remained obscure. In the 1990s, a second generation of economists took up the human-capital mantle. By then, neoliberal politics was in full swing. The fact that human capital theory explicitly justified inequality was no longer a liability. Today, the fortunes of human capital theory seem to have peaked. We can see the scientific flaws by returning to William Muir’s chicken experiment. I have already told you about his psychopathic chickens, created by breeding the most productive hens. But I have not told you about his alternative trial. In it, he bred the most productive group of chickens. The result was an astonishing increase in egg-laying productivity. The reason this group selection worked is that chickens are social animals. That means productivity is influenced by the social environment. By selecting productive groups, Muir selected for egg-laying ability, but also for sociality. The resulting social hens flourished together. Human capital theory supposes that income stems from productivity, and that this productivity is an isolated trait of the individual. When we expose the realities of power (a social trait), we undermine the legitimacy of the social order. Blair Fix: Human Capital Theory RWER95 (pdf) 10.5 Efficiency Klees Much has been written about the failure of neoclassical economic theory (NCT), so I won’t belabor the point, but I do want to highlight what is too rarely said – that the central concept of NCT, economic (Pareto) efficiency is empty in theory and practice. The great feat of neoclassical economics has been to convince people that there is a vantage point to view society, separable from concerns with equity and distribution. This vantage point, defined as economic efficiency, supposedly allows one to see if society as a whole is better off, such that decisions to produce a particular array of goods and services could be made in the interests of everybody, irrespective of how little one had, thus separating efficiency decisions from equity ones. However, if prices are not defined according to the exact dictates of what economists call “perfect competition,” then private profitability tells us nothing about the comparative social advantages and the consequent “efficiency” of producing, let’s say, more yachts for rich people instead of more rice and beans for poor people. Similarly, to argue that the allocation of resources can be “efficient” even if half the world is starving to death is ridiculous, but that is exactly what neoclassical economics says. I find this legerdemain of inventing a concept of efficiency separate from equity, based on a completely unreal, obviously untrue, abstraction, is absurd on the face of it. If the absurdity of this framework is not obvious, one only has to look at what NCT calls “second best theory.” The “first best” world is that of perfect competition; “second best” refers to a world with at least one “imperfection,” say, one monopoly in a world that was otherwise perfectly competitive. Second-best theory essentially asks: “If we don’t live in the first-best world of perfect competition but have, let’s say, only one imperfection in an otherwise perfect world, what are the results?” It turns out, reluctantly admitted by neoclassical economists – second best is their own theory, not a plot by critics – that with just one imperfection, there are ripples so that all market prices become distorted, and Adam Smith’s famous invisible hand is no longer a good guide to the social interest, and the system is no longer efficient — nor is there even any sense of whether it is close to efficiency. In the real world of multiple imperfections – where none of the assumptions of perfect competition hold – even if the neoclassical concept of efficiency had some meaning in theory, in practice, it is an abysmal failure, a completely empty idea. The implication of my critique of NCT is that if economic efficiency is meaningless, neoclassical economics is useless. There is no reason to stubbornly hang on to NCT and its justification for capitalism in the way that even critical neoclassical economists like Krugman, Reich, Rodrik, and Stigltiz do. Klees (2021) Neoclassical Economics is Dead. What Comes Next? 10.6 Socialist Alternative to Human Capital Theory The liberation of learning from the tyranny of earning. Mehta ‘THE DEATH OF HUMAN CAPITAL?: Its Failed Promise and How to Renew It in an Age of Disruption’, by Phillip Brown, Hugh Lauder, and Sin Yi Cheung, disputes the theory behind one of the strangest features of the past 40 years of neoliberal economics, one rarely tackled so directly. This is human capital theory (HCT), which tends to shift the responsibility for good jobs and wages from business to higher education. At one time, a company that laid off hundreds or thousands of workers would be admitting its managerial failure and incompetence; in the 1980s and ’90s, the mass layoff came to signal instead the kind of decisive cost-cutting that would pump up stock price. Disposing of workers was just the first step. The next was to demand that they embark on a journey of self-improvement. This would make them employable in the “new economy” for the “jobs of the future.” Public officials stopped expecting that firms maintain employment; they wrote tax law that favored companies that sent union jobs overseas. The once and future worker could only be worthy of new jobs if the country’s colleges — two-year and four-year, assisted by a new collection of for-profit colleges and training companies — acquired the proverbial “laser focus” on job-ready skills. HCT, which appealed to conservatives and liberals alike, had become the master paradigm of the “information society” and the “knowledge economy” by the early 1960s. Forerunners of the theory had drawn the interest of some 19th-century economic thinkers, such as John Stuart Mill and Alfred Marshall (who identified what he called “personal capital”), and crucial postwar contributions came from a set of Chicago School economists: Milton Friedman, Theodore W. Schultz, Jacob Mincer, and, most insistently, Gary Becker. HCT was taken up by university presidents like Clark Kerr to explain why universities were at the heart of the postwar economy, where new wealth came from knowledge as human capital and not just physical capital or physical labor. The theory was adopted by New Democrats in the 1980s and ’90s, who liked the claim that knowledge work, in alliance with technology, turbocharged the creation of value and wealth compared to regular industrial labor. In the apparent ebbing of direct colonial extraction, knowledge work was to keep the West on top of the economic food chain, relegating the Global South to manual labor on products conceived and designed in London, Stuttgart, and Cupertino. In his influential 1991 book The Work of Nations, Robert B. Reich (Bill Clinton’s first secretary of labor) synthesized an evolutionary theory of the economy in which production workers in the United States would (and should) be replaced by that more advanced employee, the “symbolic analyst,” who works with the mind on numbers, words, and ideas. HCT made the collective level of education — defined as its fit with advancing technology — the prime mover of contemporary capitalism. Getting this fit between education and technology became a main objective of public policy. HCT emerged near the end of a century of high economic growth — a period during which, as Robert J. Gordon documents in his 2016 book, The Rise and Fall of American Growth, productivity and wages grew mostly because of revolutionary inventions — think electric grids, cars, telephones, and elevators — coupled with Fordist and then New Deal approaches to the distribution of the resulting economic gains. Promoting education principally as human capital is not simply narrow-minded but increasingly dangerous. Using the monetary rewards of education to promote and finance it becomes an increasingly bad idea when those monetary rewards fail to materialize. Having done serious damage to HCT, the authors offer a new kind of human capital theory. This is somewhat confusing, unless we accept that their progressive educational theory, grounded in John Dewey, among others, seeks to combine educational contributions to production with human development rather than eliminating the productivity dimension. The emphasis of the new human capital is on education for personal growth, nurturing a holistic relation between knowing and doing. But if their theory makes personal growth the central goal, it is still a human capital theory, whose focus includes without being limited to the “economic productivity of the human being.” The authors are invoking the capabilities approach rooted in heterodox economics as practiced by Amartya Sen and Martha Nussbaum, and also socially grounded political theory, philosophy, and the psychology of labor and creativity, whose key figures include Hannah Arendt, W. E. B. Du Bois, and C. L. R. James, as well as Aristotle and Karl Marx. The authors are trying to construct a socialist alternative to human capital theory. Replace HCT with an understanding of education and labor as related but distinct modes of human empowerment. Education should focus on the full development of all capabilities of each individual in the whole population. Schooling must be universal, and in the 21st century postsecondary education should be too (though modes and structures will vary widely). Education must develop the whole range of capabilities and not just those with manifest relevance to jobs and wages; capabilities go well beyond the life of homo economicus. Which capabilities to emphasize will vary from person to person: a student who loves history, public policy, or set design should receive systematic education in the deep content and skills of history, journalism, and theater — and not, as now, given a smattering while being advised to be more interested in and better at math, coding, or accounting. Learning should be seen as central to the individual’s entire life and not mainly as an investment in a wage. Individual capabilities are fundamentally social, derived from overall social intelligence and embedded in social relations that mix labor and learning on an ongoing basis. The authors’ new-HCT model would lead to a double transformation. The first is “the liberation of learning from the tyranny of earning.” Business has used HCT to cut education down to its own size, reducing social, cultural, and scientific knowledge that would serve the world in long-range and unpredictable ways. ‘The Death of Human Capital?’ points toward a world beyond human capital theory, which has functioned as a (failed) alternative to industrial policy, impaired equitable social development, and constrained the power of education. Other authors should build on the project under way here. Mehta (2021) A Socialist Alternative to Human Capital Theory? 10.7 Production Function Fix Friedman’s famous ‘F-twist’, in which he argued that a theory’s assumptions are irrelevant. All that matters, Friedman claimed, is that the theory makes accurate predictions. Friedman’s F-twist gets dubious assumptions off the hook. But there is still the problem of predictions. How do you ensure that your theory is consistent with the evidence? Here, neoclassical economists have hit upon a tidy trick: frame your theory in terms of an accounting identity. Since the identity is true by definition, any ‘test’ of the theory will come out in your favor. When neoclassical economists test their theory of income (the theory of marginal productivity), they invoke an accounting identity. They correlate two related forms of income (usually sales and wages) and then call one of the incomes ‘productivity’. Since they always find a correlation, they always ‘confirm’ their theory of income. Nifty! Then there’s the neoclassical theory of economic growth. The theory assumes that economic output is governed by a ‘production function’ that dictates how inputs of capital, labor and ‘technological progress’ are transformed into economic output. And guess what … this approach seems to have overwhelming empirical support. The problem, pointed out by Anwar Shaikh, is that the production function is actually a re-arrangement of a national-accounting identity. The production function ‘works’ because it is true by definition. Nice! Fix (2021) The Truth About Inflation 10.8 Keen Critique Bichler Nitzan Neoclassical economics is the of- ficial scientific underpinning of capitalism as well as its main ideological defence, and according to Keen, it fails in both tasks. Contrary to received opinion, neoclassicism cannot explain capi- talism – either in detail or in the aggregate – and the policies it prescribes do not support but undermine the very system it defends. The book focuses on three key issues: the bizarre neoclassical perspective that money, credit and debt do not matter for the macroeconomy; the neoclassical insistence that the economy’s complex, nonlinear turbulences are best explained in linear, self-equilibrating terms; and the fact that neoclassicists have hijacked the economics of climate change, using patently false assumptions to justify do-nothing policies with untold future consequences. In the neoclassical universe, government is bad business. But the book also has one important limitation: it is about economics. Keen offers to replace neoclassical dogma with a new way of thinking, researching and en- gaging with the economy. And while we agree that neoclassicism is a religion dressed as a sci- ence, in our view, what should come in its stead is not a different type of economics, but a new theory of capitalism more broadly. This isn’t semantic nit-picking. All economic theories – including neoclassicism – engage with non-economic entities and forces. They all agree, willingly or reluctantly, that politics, sociology, anthropology, psychology, international relations and other aspects of society affect the economy. But these effects, whether supportive or distortive, are assumed external to the economy proper. And this assumption is pivotal. Although the effects of these so-called external factors alter economic outcomes, they leave the economic categories themselves intact. And this bifurcation, we argue, is the Achilles’ heel of all economic theories, orthodox and heterodox, old and new. In our view, capitalism is not an economic system, but a conflictual mode of power. Those who rule this mode of power – its dominant capitalists, politicians, mainstream academics, opinion makers and the various organizations they control – make every effort to conceal its power features. This is why neoclassical economics, beholden to its masters, can never be a science. But the problem besieges every and any economic theory that keeps power external to its basic categories. In our opinion, it is only when the study of capitalism substitutes for the narrow understanding of its economy that power can assume centre stage to reveal what economics is structured to conceal. Bichler Nitzan (2021) on Stev Keen’s Manifesto (pdf) "],["neo-liberal-economics.html", "11 Neo-liberal Economics 11.1 Austerity 11.2 Washinton Consensus 11.3 Neoliberalism vs Capitalism", " 11 Neo-liberal Economics Neoliberalism is the ideology that advocates market primacy of social coordination. The neoliberal solution to climate change is to hope that somehow it will become profitable to save the planet. This will not work. (@ExistentialComics) Because neoliberalism has granted markets primacy, and because markets are vulnerable to large-scale runaway loops, neoliberalism is effectively a runaway feedback loop. 11.1 Austerity Somers on Polanyi There is too much public discourse, even within the Democratic Party, that accepts and even propagates the right-wing propaganda that a restoration of economic growth requires austerity and greater deference to the needs of business. The reality is that austerity usually results in rent-seeking behavior, with the consequence of further stagnation and crises rather than productive investment. Polanyi teaches us that periods of prosperity and rising living standards, by contrast, were a direct result of democratic gains in politics and civil society. The greatest prosperity in living memory in Europe and the United States came during the social democratic moment—in the 1950s and 1960s—when the constraints on business were the greatest. In short, more democracy and more economic justice are the necessary foundations for the path to socialism and a more vibrant, prosperous, and sustainable economy. Somers (2023) The Return of Karl Polanyi 11.2 Washinton Consensus John Williamson coined the term “Washington Consensus” to refer to a set of ten economic policies and reforms that received widespread support at the time. These policies included - maintaining fiscal discipline, - reordering public spending priorities (from subsidies to health and education expenditures), - reforming tax policy, - allowing the market to determine interest rates, - maintaining a competitive exchange rate, - liberalizing trade, - permitting inward foreign investment, - privatizing state enterprises, - deregulating barriers to entry and exit, and - securing property rights. Williamson was writing in the context of Latin America as it was emerging from the debt crisis of the 1980s. His list of policies was not proscriptive but descriptive of what he thought various Washington-based institutions, such as the US Treasury, the International Monetary Fund, the World Bank, and various think tanks, agreed would stabilize and restore growth in the region. Williamson’s original conception indicated the general direction in which policy should move, away from a heavily statist approach while retaining an important regulatory role for government. Another version came to represent an extreme market-fundamentalist neoliberal approach that simplified economic policy to “stabilize, liberalize, and privatize” with minimal government, all of which was far from Williamson’s original intent. Critics charged that the Washington Consensus ignored the problems associated with rising inequality and even encouraged the weakening of social safety nets. A series of financial crises—the tequila crisis in 1994–95, the Asian crisis in 1997–98, and the Russian crisis of 1998—further damaged the reputation of Washington Consensus–type policies. Of course, critics of the Washington Consensus usually did not argue that emerging markets should pursue policies of fiscal indiscipline, high inflation, financial repression, trade protectionism, overvalued exchange rates, more nationalization of business, and the like. Rather, they tended to argue that the original list of ten policies was incomplete and that additional policies were needed to improve economic performance. Williamson’s list was also very general, leaving ample room for debate as to how far to go in achieving those policy objectives. But as we mark the thirtieth anniversary of John Williamson’s initial discussion of the Washington Consensus, it is important to recognize that a growing body of recent research suggests that the Consensus has produced tangible benefits while unorthodox populist policies have entailed significant economic costs. A key challenge for policymakers is to ensure that the benefits of economic reform are widely shared so that the divisions that lead to economic populism do not arise and erase those gains. Peterson Imagine writing ‘Washington Consensus really does work’ at the end of a year when central banks bought all bonds issued by fiscal authorities in high income countries, so said fiscal authorities can put safety nets under both capital and labour. Including Nicaragua is problematic to start with: first Ortega regime had to contend with an internal war against paramilitaries (Contras) financed by US in the famous Iran-Contras affair. You’d think the paper would mention that. Guys, the Sandinista Revolution was distracted from growth outcomes by Ronald Reagan bombing them for refusing to say ‘yes uncle, mi patria tu patio’ Ironically, Ortega’s rein since 2007 is far more consistent with the paper’s definition of ‘populism’, but it’s a Washington Consensus, pro-market populism, so of course, we go for the civil war period. An entire subsection on infant mortality under Ortega vs ‘synthetic Nicaragua’ that does not mention the war! Of course, synthetic Nicaragua is one that the US empire doesnt bomb. Gabor (twitter comment) Un thread à lire absolument pour voir le type d’analyse défendu par certains économistes américains universitaires réputés. Où inventer des pays “fictifs” entièrement à sa main est censé permettre de “prouver” le bénéfice de certaines lignes idéologiques. (@NBarreyre) Alves Decades of research have documented the devastating impacts of the Washington Consensus in the developing world. Yet revisionist accounts of this story have emerged in recent years. Remarkable amongst these, a recent blog post by the Peterson Institute for International Economics – “Washington Consensus stands the test of time better than populist policies” – draws on research that is jaw-droppingly ideological and flawed. For decades, mainstream and heterodox economists broadly agreed that the Washington Consensus failed (Stewart 1995, Krueger 2004, Mkandawire 2005). Debt-crisis ridden developing countries that implemented the reforms associated with privatization, liberalisation and deregulation in the 1980s and 1990s tended to see an increase in poverty along with worsening health and educational outcomes. This led to the 1980s and 1990s being dubbed the ‘lost decades’ of development (Easterly 2001) and ultimately paved the way for the the post-Washington Consensus and pro-poor policies (Saad-Filho 2011). But this is about to change. New methods that ‘produce credible counterfactuals in case studies’, turn the conventional wisdom of the Washington Consensus failure on its head (Marrazzo and Terzi 2017, Absher et al. 2020, Grier and Grier 2021). Essentially, the counterfactual approach involves first creating fictitious or synthetic countries, whose policy makers chose the opposite policy trajectory, and then testing whether the Washington Consensus package works better than the alternative. The results, the PIIE blog informs us, stack up for the Washington Consensus: countries adopting WC policies are shown to (eventually) be better off in GDP per capita terms. In contrast, left-wing populists – of the Latin American pedigree – hurt their economies by throwing the Washington Consensus policies out with the neoliberal bathwater. If one unpacks its mechanics carefully, the counterfactual approach turns out to be a thinly-veiled ideological attempt to whitewash the Washington Consensus, to resurrect its key tenets: that minimising the footprint of the state is the right policy choice in health or education, that macroeconomic policy should mean inflation targeting by central banks not active fiscal policy by elected politicians, that state-owned companies are all white elephants in urgent need of privatization, that trade unions harm labour markets. How does one empirically create a fictitious country? The synthetic control method predicts a ‘no Ortega’ growth/infant mortality path by creating a pool of ‘donor’ countries and calibrating their relative contribution to a synthetic Nicaragua such that the pre-Ortega growth or infant mortality path is close to actual Nicaragua. The Washington Counterfactual thus creates a synthetic Nicaragua composed of 23% Chile, 54% Honduras, 9% Mexico, 8% Norway, and 7% the US. Or, to bring a historical touch to the method, synthetic Nicaragua, like a neoliberal Frankenstein, consist of 7% country bombing Nicaragua (US), 54% country used by the CIA/US to bomb Nicaragua (Honduras), 23% country where Washington Consensus was being implemented by Chicago Boys and a military dictatorship (Chile), 8% country never analytically paired with the Washington Consensus (Norway), and Mexico. It is this synthetic Nicaragua where per capita GDP would have been 5.000 USD dollars higher, a Nicaragua that the US empire does not bomb. Revisionist accounts of the Washington Consensus matter because the pandemic has revived the debate around the role that the state should play in the economy. If anything, this is a powerful reminder that all economics is political, however much some hide it behind new or ‘sophisticated’ econometric techniques. Alves on Peterson 11.3 Neoliberalism vs Capitalism Hickel Neoliberalism is not the disease. It is just a symptom of the disease. The disease is capitalism. First, we need to understand what capitalism actually is. Under capitalism, the purpose of production is not primarily to meet human needs. This is no generic economy. Rather, the purpose is to maximize and accumulate profit. That is the core objective. Toward this end, capital seeks to cheapen inputs—labour and nature—as much as possible. For most of its history, capital brutally exploited workers in the core economies, and relied on imperialism to guarantee a study supply of cheap labour and resources in the global South. But this arrangement came under threat after WWII. Labour movements in the core succeeded in winning better wages, better working conditions, and a wide-range of public services: healthcare, housing, education, transit… Meanwhile, in the South, anti-colonial movements overthrew imperialism and began introducing socialist reforms: nationalizing resources, improving wages, building public services, and using tariffs, capital controls and industrial policy to achieve economic sovereignty. This radical turn dramatically improved the lives of working people, North and South. But the new regime of fair wages and resource prices made capital accumulation in the core increasingly untenable, triggering a crisis for elites in the 1970s. As it turns out, capitalism cannot function for very long under conditions of worker justice and decolonization. For capitalists in the core, it was clear that something had to change. The core states faced a choice: either they could accept the fair wages and decolonization, abandon capital accumulation and shift to a post-capitalist economy… or they could attack wages and somehow re-impose the imperial arrangement. They opted hardcore for the latter. At home, they dismantled the unions and shredded public services. They slashed all manner of regulations and protections, in a desperate bid to restore the conditions for capital accumulation. Today we know this as neoliberalism. Neoliberalism was imposed even more brutally across the South, through structural adjustment programs. They reversed the socialist reforms of the anti-colonial era, cut wages and resource prices, and destroyed economic sovereignty… subordinating Southern economies once again. This was not some kind of “mistake”. Not just bad theory. Neoliberalism was imposed in order to restore the conditions for capital accumulation. It was an orchestrated backlash against the successes of the labour movement and the anti-colonial movement. This is why, despite 40 years of data on how destructive neoliberal policies are, we are still stuck in this nightmare. We are stuck because the obvious solution—worker justice, regulation, and economic sovereignty in the South—is inimical to capital accumulation in the core. There is a way out of this nightmare, and that is to abandon capital accumulation as an objective and transition to a post-capitalist economy. Neoliberalism is just a symptom. If we want to advance we need to deal with the underlying structural problem. *steady, that is. We can have a democratic economy organized around meeting human needs at a high standard, where production is socially just and ecologically regenerative. Such a system is possible, but it requires transitioning out of capitalism. Hickel (2022) Twitter thread ThreadReader "],["physiocrates.html", "12 Physiocrates", " 12 Physiocrates Milanovic It is well-known in the history of economic thought that the founders of political economy, the Physiocrats and Quesnay in particular, thought that only agriculture is productive and manufacturing is “sterile”. Their use of the word “sterile”, and insistence on it, is unfortunate because the reality of what they argued is more sensible and sophisticated. What they meant by “sterility” of manufacturing is that the price of manufactured goods decomposes into depreciation of capital, subsistence wage, “normal” return to the capitalist (which they, rather generously, assumed to be 10% per year) and even “recompense” for entrepreneurship. So why is it different from what economists believe today? In effect we use exactly the same components. But for the Physiocrats there was no surplus (on top of these four components which they saw as simple inputs) which could be taxed to provide income for the elite: the people who do not directly participate in the process of production, namely landlords, clergy and government officials. In a certain way, it was quite sensible: one cannot tax subsistence wages, nor can he tax profit that is at a “normal” rate if he believes that driving it below that rate will result in no production being undertaken at all. However, absence of surplus in manufacturing was at odds with what was observed in France at the time. As Georges Weulersse, the author of the most detailed analysis of physiocracy (see below), explains, large fortunes—by definition made of accumulated surpluses—existed in manufacturing. Yet Physiocrats stubbornly refused to admit that, and argued that these fortunes existed only because of special protection given to individual industrialists. Manufacturing, they held, could not produce surplus on its own without state protection. It was only in agriculture that existed a surplus that could be taxed: ground rent. For in agriculture, price decomposes into depreciation, return to capital advanced by the tenant-farmer, subsistence wages paid to the hired labor and…the rent. Rent is not an income necessary to bring forth output. It is (as the nice formula has it) price-determined, not price-determining. It is the only net income which can be taxed and used to maintain a civilized society that needs government for the protection of property and administration of justice, and clergy for the provision of spiritual sustenance. Physiocrats, quite consistently with this view, argued in favor of policies that would raise agricultural production through longer land-leases (so that tenant farmers have incentive to invest in land improvement), freedom of movement of grain (laissez-passer) and freedom of exports. Such measures would increase both the quantity of agricultural output and the relative price of food, and therefore doubly yield a greater net product, the goal of economic activity as defined by them. This is, in essence, the logic behind the view that only agriculture is productive, which, without looking at it more closely, seems strange to the modern ear. But thinking of it, poses the all-important question: what is net income? There is no general answer to that question. It depends on what the social structure is, and what is the goal of economic activity. Incomes that Physiocrats disregarded, i.e., wages, depreciation, interest, and entrepreneurial profit, are all parts of the gross value added as presently defined. Their and our components were not different. It is just that they were not interested in the components, like wages, that appear to us to be a valuable goal of economic activity. Some suggested readings on Physiocrats: Georges Weulersse, Le movement physiocratique en France, de 1750 à 1770. Paris: Maison des Sciences de Homme, Editions Mouton, 1910. Available at http://archive.org/details/lemouvementphysi01weuluoft. (A monumental two-volume discussion of the intellectual milieu before the Revolution within which the movement developed, its rise and fall, and its ideology.) Gianni Vaggi, The Economics of François Quesnay, Durham, N.C : Duke University Press, 1987. (Excellent and sophisticated analysis of the physiocracy enlightened by Sraffianism. Ronald L. Meek, Economics of Physiocracy, Routledge, 1962. (A standard reference text with translation of selected writings, including Le tableaux économique and brilliant introduction by the editor), Marguerite Kuczynski and Ronald L. Meek (edited with new material, translations and notes), Quesnay’s “Tableau économique”, London: MacMillan for the Royal Economic Society and the American Economic Association, 1972. (Le Tableau in all its grandeur and complexity with the commentary.) Milanovic (2023) Net economic output in history: Why we work? Ideology behind economic accounting "],["productivism.html", "13 Productivism", " 13 Productivism Rodrik There are signs of a major reorientation toward an economic policy framework that is rooted in production, work, and localism instead of finance, consumerism, and globalism. It might just turn into a new policy model that captures imaginations across the political spectrum. A new bipartisan consensus may be emerging around “productivism,” which emphasizes the dissemination of productive economic opportunities throughout all regions and all segments of the labor force. Unlike neoliberalism, productivism gives governments and civil society a significant role in achieving that goal. It puts less faith in markets, is suspicious of large corporations, and emphasizes production and investment over finance, and revitalizing local communities over globalization. Productivism also departs from the Keynesian welfare state by focusing less on redistribution, social transfers, and macroeconomic management and more on supply-side measures to create good jobs for everyone. And productivism diverges from both of its antecedents by reflecting greater skepticism toward technocrats and expressing less knee-jerk hostility to economic populism. Examples include the embrace of industrial policies to facilitate the green transition, rebuild domestic supply chains, and stimulate good jobs; blaming large corporate profits as a culprit behind inflation, and refusing (so far) to revoke former President Donald Trump’s tariffs against China. When the administration’s most senior economist, Secretary of the Treasury Janet Yellen, extols the virtues of “friend-shoring” – sourcing supplies from US allies – over the World Trade Organization, we know the times are changing. In those instances in which the market’s most efficient outcome is one that’s bad for our people what we need is targeted industrial policy to further the common good. “State capacity” is one of its main planks, emphasizing that governments’ ability to provide public goods is important for a healthy economy. “Pro-worker policies” and “the encouragement, through government policy, of domestic production.” Rodrik (2022) Productivism "],["steady-state-economics-sse.html", "14 Steady State Economics (SSE) 14.1 Herman Daly", " 14 Steady State Economics (SSE) 14.1 Herman Daly Point Policy Summary From “A Steady-State Economy,” by Herman E. Daly School of Public Policy, University of Maryland, College Park MD 20742 USA Cap-auction-trade systems for basic resources. Cap limits to biophysical scale according to source or sin k constraint, whichever is more stringent. Auction captures scarcity rents for equitable redistribution. Tra de allows efficient allocation to highest uses. Ecological tax reform—shift tax base from value added (labor and capital) and on to “that to which value is added”, namely the entropic throughput of resources extracted from nature (depletion), through the economy, and back to nature (pollution). Internalizes external costs as well as raises revenue more equitably. Prices the scarce but previously unpriced contribution of nature. Limit the range of inequality in income distribution—a minimum income and a maximum income. Without aggregate growth poverty reduction requires redistribution. Complete equality is unfair; unlimited inequality is unfair. Seek fair limits to inequality. Free up the length of the working day, week, and year—allow greater option for leisure or personal work. Full-time external employment for all is hard to provide without growth. Re-regulate international commerce—move away from free trade, free capital mobility and globalization, adopt compensating tariffs to protect efficient national policies of cost internalization from standards-lowering competition from other countries. Downgrade the IMF-WB-WTO to something like Keynes’ plan for a multilateral payments clearing union, charging penalty rates on surplus as well as deficit balances— seek balance on current account, avoid large capital transfers and foreign debts. Move to 100% reserve requirements instead of fractional reserve banking. Put control of money supply and seigniorage in hands of the government rather than private banks. Enclose the remaining commons of rival natural capital in public trusts, and price it, while freeing from private enclosure and prices the non rival commonwealth of knowledge and information. Stop treating the scarce as if it were non scarce, and the non scarce as if it were scarce. Stabilize population. Work toward a balance in which births plus immigrants equals deaths plus out-migrants. Reform national accounts—separate GDP into a cost account and a benefits account. Compare them at the margin, stop growing when marginal costs equal marginal benefits. Never add the two accounts. Herman Daly (2008) Steady State Economy (Sustainable Development Commision) (pdf) "],["spatial-economics.html", "15 Spatial Economics 15.1 Urban Economics 15.2 History of Urban Economics 15.3 Regional Economics", " 15 Spatial Economics Agglomoration Economics (Ongoing Research Programme) HSCIF 15.1 Urban Economics When and why did the expertise associated with economics as an academic discipline become so highly valued in the world of public policy? The embedding of agglomerationism within the thinking of policy-makers and governmental institutions provides a fascinating example of a broader shift towards the growing impact of economic expertise, and indeed of individual economists, on policy-making. This focus sits within a wider field of study which is interested in the complex roles that economists have at times played – as public intellectuals, policy experts and academic specialists. How different kinds of analytical tools and a particular style of economic reasoning made their way into the world of elite decision-making is a major theme of interest for many historians and social scientists. So too is the related question of how quantification (testable theoretical hypotheses, measurement technique and indicators, as well as decision-models) has over the last few decades gained ascendancy in policy circles. History of Urban Policy Expertise 15.1.1 Expertise ExpertiseunderPressure What is the role of experts in understanding social change? Expert judgment today is both intensely sought out, across private and public spheres, and also intensely criticised and derided with well-publicised failures to predict various high profile social and natural phenomena. Does the problem lie with the very idea that objective expertise about complex processes is attainable? Or does it stem from the way that expert judgment is developed and communicated? Or, perhaps it reflects the diminished standing of experts and expert knowledge in democratic and pluralistic societies? To explore these questions, we propose three case studies in which expert judgment is both consequential and controversial. They are the UK Government’s emergency response, the use of agglomeration theory in city planning, and deep philosophical controversies about the possibility and objectivity of social science. These cases differ in scope and focus but they enable us to analyse four distinct features of legitimate expertise: sensitivity to temporal scale, translatability in space, ambivalence about precision, and moral responsibility. The overarching goal of the project is to establish a broad framework for understanding what makes expertise authoritative, when experts overreach, and what realistic demands communities should place on experts. CRASSH Expertise under Pressure Programme 15.1.2 Trusting Science Bennett Trust is necessary for many kinds of policy, particularly where that policy requires citizens to comply with rules that come at significant cost, and coercion alone would be ineffective. What is distinctive about our pandemic policies is that they depend not just on public trust in policy, but public trust in the science that we are told informs that policy. When public policy claims to follow the science, citizens are asked not just to believe what they are told by experts, but to follow expert recommendations. While ministers defer to scientists, those same scientists have been eager to point out that their role is exclusively advisory. We are still being asked by the government to trust in recommendations provided by experts, even if the government is not being led by evidence in the way it would have us believe. The communications strategy may not be honest. Public trust in science is both a necessary and desirable feature of an effective public health response to the pandemic. But it is desirable only insofar as it is well placed trust. What makes trust in experts reasonable, when it is? A perceived threat to knowledge about a range of basic facts that most of us don’t have the resources to check for ourselves. If an expert tells me that something is the case this is enough reason for me to believe it too, provided that I have good reason to think that the expert in question has good reason to believe what they tell me. Is it still reasonable to trust science when it doesn’t just provide policy-relevant facts, but leads the policy itself? Knowledge regarding the relevant facts might not reliably indicate ability to reason well about what to do in light of the facts. Well-placed trust in the recommendation of an expert is more demanding than well-placed trust in their factual testimony. A good reason for an expert to think I should do something is not necessarily a good reason for me to do it. This is because what I value and what the expert values can diverge without either of us being in any way mistaken about the facts of our situation. One helpful measure to show the public that a policy does align with their interest is what is something called expressive overdetermination: investing policy with multiple meanings such that it can be accepted from diverse political perspectives. Reform to French abortion law is sometimes cited as an example of this. After decades of disagreement, France adopted a law that made abortion permissible provided the individual has been granted an unreviewable certification of personal emergency. This new policy was sufficiently polyvalent to be acceptable to the most important parties to the debate; A second helpful measure, which complements expressive overdetermination, is to recruit spokespersons that are identifiable to diverse groups as similar to them in political outlook. This is sometimes called identity vouching. The strategy is to convince citizens that the relevant scientific advice, and the policy that follows that advice, is likely not to be a threat to their interests because that same consensus is accepted by those with similar values. Expressive overdetermination and identity vouching are ways of showing the public that a policy is in their interests. Whether they really are successful at building public trust in policy, and more specifically in science-led policy, is a question that needs an empirical answer. What I have tried to show here is that we have good theoretical reasons to think that such additional measures are needed when we are asking the public not just to believe what scientists tell us is the case, but to comply with policy that is led by the best science. Public trust in science comes in at least two very different forms: believing expert testimony, and following expert recommendations. Efforts to build trust in experts would do well to be sensitive to this difference. [Bennett - Trusting the experts take more than belief(Blog Post)]https://hscif.org/trusting-the-experts-takes-more-than-belief/) 15.2 History of Urban Economics Cherrier and Rebours The field of ‘Urban Economics’ is an elusive object. That economic phenomena related to the city might need a distinctive form of analysis was something economists hardly thought about until the early 1960s. In the United States, it took a few simultaneous scholarly articles, a series of urban riots, and the attention of the largest American philanthropies to make this one of the hottest topics in economics. The hype about it was, however, short-lived enough so­­­ that, by the 1980s, urban economics was considered a small, ‘peripheral’ field. It was only through the absorption into a new framework to analyze the location of economic activities – the ‘New Economics Geography’ – in the 1990s that it regained prominence. Understanding the development of urban economics as a field, or last least the variant which originated in the US and later became international, presents a tricky task. This is because the institutional markers of an academic field are difficult to grasp. A joint society with real estate economists was established in 1964, and a standalone one in 2006; a journal was founded in 1974, with an inaugural editorial which stated that: “Urban economics is a diffuse subject, with more ambiguous boundaries than most specialties. Situated within a master-discipline (economics) that is often described as exhibiting an articulated identity, clear boundaries with other sciences and strict hierarchies, urban economics is an outlier. There is, however, one stable and distinctive object that has been associated with the term ‘urban economics’ throughout the 1970s, the 1980s, the 2000s and the 2010s: the Alonso-Muth-Mills model (AMM). It represents a monocentric city where households make trade-offs between land, goods and services, and the commuting costs needed to access the workplace. The price of land decreases with distance from the city center. The model was articulated almost simultaneously in William Alonso’s dissertation, published in 1964, a 1967 article by Edwin B. Mills, and a book by John Muth published in 1969. This trilogy is often considered as a “founding act” of urban economics. Agglomeration In 1956, William Alonso moved from Harvard, where he had completed architecture and urban planning degrees at the University of Pennsylvania. He became Walter Isard’s first graduate student in the newly founded department of “regional science.” He applied a model of agricultural land use developed 150 years earlier by the German economist Johann Von Thünen to a city where all employment is located in a Central Business District. His goal was to understand how the residential land market worked and could be improved. His resulting PhD, Location and Land Use, was completed in 1960. Around that time, young Chicago housing economist Richard Muth spent a snowstorm lockdown thinking about how markets determine land values. The resulting model he developed was expanded to study population density. And a book based on it was published a decade later: Cities and Housing. Drafts of Alonso and Muth’s work reached inventory specialist Edwin Mills in 1966, while he was working at the RAND corporation, and trying to turn models describing growth paths over time into a model explaining distance from an urban center. His “Aggregative Model of Resource Allocation in a Metropolitan Area” was published the next year. This new set of models immediately drew attention from a wide array of transportation economists, engineers and geographers concerned with explaining the size and transformation of cities, why citizens chose to live in centers or suburbs, and how to develop an efficient transportation system. The economists included Raymond Vernon and Edgar Hoover, whose study of New York became the Anatomy of the Metropolis; RAND analyst Ira Lowry, who developed a famous spatial interaction model; spatial and transportation econometrician Martin Beckman, based at Brown; and Harvard’s John Kain, who was then working on his spatial mismatch hypothesis and a simulation approach to model polycentric workplaces. Through the early works of Brian Berry and David Harvey, quantitative urban geographers also engaged with these new urban land use models. But the development of a new generation of models relying on optimization behavior to explain urban location was by no mean sufficient to engender a separate field of economics. Neither Alonso, who saw himself as contributing to an interdisciplinary regional science, nor Muth, involved in Chicago housing policy debates, cared much about its institutionalization. But both were influenced and funded by men who did. Muth acknowledged the influence of Lowdon Wingo, who had authored a land use model. Together with Harvey Perloff, a professor of social sciences at the University of Chicago, they convinced the Washington-based think-thank Resource for the Future to establish a “Committee for Urban Economics” with the help of a grant by the Ford Foundation. The decision was fueled by urbanization and dissatisfaction with the urban renewal programs implemented in the 1950s. Their goal was to “develop a common analytical framework” through the establishment of graduate programs in urban economics. Their agenda was soon boosted by the publication of Jane Jacobs’ The Death and Life of Great American Cities, and by growing policy interest in the problems of congestion, pollution, housing segregation and ghettoization, labor discrimination, slums, crime and local government bankruptcy, and by the stream of housing and transportation acts which were passed in response to these. The Watts riots, followed by the McCone and Kerner commissions, acted as an important catalyst. The Ford Foundation poured more than $ 20 millions into urban chairs, programs and institutes through urban grants awarded to Columbia, Chicago, Harvard and MIT in 1967 and 1970. The first round of funds emphasized “the development of an analytical framework”, and the second sought “a direction for effective action.” As a consequence of this massive investment, virtually every well-known US economist turned to urban topics. At MIT, for instance, Ford’s money was used to set up a two-year “urban policy seminar,” which was attended by more than half of the department.The organizer was welfare theorist Jerome Rothenberg, who had just published a book on the evaluation of urban renewal policies. He was developing a large-scale econometric model of the Boston area with Robert Engle and John Harris, and putting together a reader with his radical colleague Matt Edel. Department chair Carry Brown and Peter Diamond were working on municipal finance. Robert Hall was studying public assistance while Paul Joskow examined urban fire and property insurance. Robert Solow developed a theoretical model of urban congestion, published in a 1972 special issue of the Swedish Journal of Economics, alongside a model by taxation theorist Jim Mirrlees investigating the effect of commuter and housing state tax on land use. Solow’s former student Avinash Dixit published an article modeling a tradeoff between city center economies of scale and commuting congestion costs in another special issue on urban economics in the Bell Journal the next year. A survey of the field was also published in the Journal of Economic Literature, just before the foundation of the Journal of Urban Economics in 1974. Segregation But the publication of a dedicated journal, and growing awareness of the “New Urban Economics” was not the beginning of a breakthrough. It turned out to be the peak of this wave. On the demand side, the growing policy interest and financial support that had fueled this new body of work receded after the election of Richard Nixon and the reorientation of federal policies. On the supply side, the mix of questions, methods and conversations with neighboring scholars that had hitherto characterized urban economics was becoming an impediment. More generally, the 1970s was a period of consolidation for the economics profession. To be considered as bona fide parts of the discipline, applied fields needed to reshape themselves around a theoretical core, usually a few general equilibrium micro-founded workhorse models. Others resisted, but could rely on separate funding streams and policy networks (development and agricultural). Urban economics was stuck. Policy and business interest was directed toward topics like housing, public choice and transportation. And, combined with the growing availability of new microdata, micro-econometrics advances, and the subsequent spread of the personal computer, this resulted in an outpouring of applied research. Computable transportation models and real estate forecasting models were especially fashionable. On the other hand, a theoretical unification was not in sight. Workhorse models of the price of amenities, the demand for housing, or suburban transportation, were proposed by Sherwin Rosen, William Wheaton and Michelle White, among others. But explanations of the size, number, structure and growth of cities were now becoming contested. J. Vernon Henderson developed a general equilibrium theory of urban systems based on the trade-off between external economies and diseconomies of city size, but in these agglomeration effects did not rely on individual behavior. Isard’s former student Masahita Fujita proposed a unified theory of urban land use and city size that combined externalities and the monopolistic competition framework pioneered by Dixit and Joseph Stiglitz, but without making his framework dynamic or relaxing the monocentric hypothesis. At a point when there was growing interest in the phenomenon of business districts – or Edge cities as journalist Joël Garreau called them, this was considered a shortcoming by many economists. General equilibrium modelling was rejected by other contributors, including by figures like Harry Richardson, and a set of radical economists moving closer to urban geographers (such as David Harvey, Doreen Massey and Allen Scott) working with neo-Marxist ideas. Renewal In the 1990s, various trends aimed at explaining the number, size, evolution of cities matured and were confronted to one another. In work which he framed as contributing to the new field of “economic geography,” Krugman aimed to employ his core-periphery model to sustain a unified explanation for the agglomeration of economic activity in space. At Chicago, those economists who had spent most of the 1980s modeling how different types of externalities and increasing returns could help explain growth – among them Robert Lucas, José Scheikman and his student Ed Glaeser – increasingly reflected on Jane Jacob’s claim that cities exist because of the spillover of ideas across industries which they facilitate. Some of them found empirical support for her claim than for the kind within-industry knowledge spillovers Henderson was advocating. Krugman soon worked with Fujita to build a model with labour mobility, trade-offs between economies of scale at the plant level and transportation costs to cities. Their new framework he was adamant to compare to Henderson’s general equilibrium model of systems of cities. He claimed that their framework enabled the derivation of agglomeration from individual behavior and could explain not only city size and structure, but also location. In his review of Krugman and Fujita’s 1999 book with Venables, Glaeser praised the unification of urban, regional and international economics around the microfoundations of agglomeration theory. He also contrasted Krugman’s emphasis upon transportation costs – which were then declining – with other frameworks focusing on people’s own movement, and began to sketch out the research program focused on idea exchanges that he would develop in the next decades. He also insisted on the importance of working out empirically testable hypotheses. The “New Economic Geography” was carried by a newly-minted John Bates Clark medalist who had, from the outset, promised to lift regional, spatial and urban economics from their “peripheral” status through parsimonious, micro-founded, tractable and flexible models. It attracted a new generation of international scholars, for some of whom working on cities was a special case of contributing to spatial economics. In the process, however, olders ties with geographers were severed, and questions that were closely associated with changing cities, like the emergence of the digital age, congestion, inequalities in housing, segregation, the rise of crime and urban riots, became less central to the identity of this field. The field lost some sort of autonomy. Most recently, Glaeser’s insistence that urban models need to be judged by their empirical fit may be again transforming the identity of urban economics. The shift is already visible in the latest volume of the series of Handbooks in Urban and Regional Science. Its editors (Gilles Duranton, Henderson and William Strange) explain that, while its previous volume (2004) was heavily focused on agglomeration theory, this one is “a return to more traditional urban topics.” And the field is now characterised not in terms of a unified, theorical framework, but with reference to a shared empirical epistemology about how to develop causal inferences from spatial data. Overall, the successive shifts in urban economists’ identity and autonomy which we describe here, were sometimes prompted by external pressures (urban crises and policy responses) and sometimes from internal epistemological shifts about what counts as “good economic science.” A key development in the 1970s was the unification around general equilibrium, micro-founded models. It is widely held that the profession is currently experiencing an “applied turn” or a “credibility revolution”, centered on the establishment of causal inference (gold) standards. How this will affect urban economics remains unclear. Cherrier and Rebours 15.2.1 Jane Jacobs Considering her contribution to economic theory may seem counter-intuitive. In addition to lacking academic credentials, she took little interest in engaging the discipline of economics. Her models were neither formal nor developed in reference to existing models. And her view of economic theory in general was dismissive. In the opening chapter of Cities and The Wealth of Nations, “Fool’s paradise,” Jacobs lays out a history of economic thought and arrives at this sweeping conclusion: “Choosing among the existing schools of thought is bootless. We are on our own.” The same dismissive stance extended to academic institutions, as she refused numerous honorary degrees from various Universities. Jacobs Externalities Some economists picked up on her insights. A type of economic externality has been derived from her detailed historical accounts of new economic activities arising from urban diversity. Chicago and Harvard urban economists Glaeser, Kallal, Scheinkman, and Shleifer credited Jacobs in 1992 for identifying cross-industry knowledge transfers, which they dubbed “Jacobs externalities.” The concept was based on Jacobs’ The Economy of Cities and posits that knowledge transfer occur between different industries, and that local competition supports economic growth. This came four years after future Nobel prize recipient Robert Lucas pointed to Jacobs’ work while investigating the external effects of human capital in his 1988 article On the Mechanics of Economic Development, although without formalizing his insight. Lucas’ endorsement earned Jacobs increasing recognition among economists over the following decades. Paul Krugman described her as a “patron saint of the new growth theory” and her unusual status was summed up by Robert Dimand and Robert Koehn who saw her as “her own distinctive kind of political economist … an exceptional instance of a woman without academic affiliation or university training achieving recognition among leading academic economists”. And a considerable literature grew up after Glaeser et al.’s piece. Despite this interest in her work, extended reassessments of her contribution to economic thought have yet to appear. The city economy model, first developed in The Economy of Cities,argues that the desirable diversification of local economic activities depends largely on the destination of goods and services entering the city’s economy. The key claim is that imports are key to economic development: they embody knowledge and allow further diversifications in the local economy, as imports are gradually replaced by local supply, and make “room” for new imports – in a similar manner to import substitution. Jacobs uses this model to stress the long-term undesirability of overspecialization derived from a focus on maximizing exports, and the importance of a large and diverse local economy – ultimately delivering a critique of comparative advantages as an organizing principle of trade. The more niches that are filled in a given natural ecology, other things being equal, the more efficiently it uses the energy it has at its disposal … That is another way of saying that economies producing diversely and amply for their own people and producers, as well as for others, are better off than specialized economies … The most elaborate study of Jacobs’ use of biological and ecological analogies is provided in mathematician and philosopher David Ellerman’s paper How Do We Grow? Jane Jacobs on Diversification and Specialization (2005). Depicting the city economy’s boundaries as an open system governed by evolutionary dynamics: “development is a conceptualized form of social learning.” Incoming goods, the products of foreign know-how, are vectors of developmental learning. And exports of commodities and services fund these imports. When imports feed into the somewhat enclaved export economy (i.e. overspecialized), they have a lesser effect then when they are dissipated in local consumption. Following Geoffrey Hodgson’s taxonomy in Economics and Evolution (1993), part of Jacobs’ system could be characterized as phylogenetic and non-consummatory, that is, as exhibiting an open-ended process of evolutionary selection among a population of firms and individuals. Jacobs targeted development schemes developed by the World Bank. She pointed to the inherent weaknesses of Robert McNamara’s development strategies for addressing “basic human needs” (literacy, nutrition, reduction in infant mortality, and health) of poor populations. She argued that because economic development is a process, it cannot be thought of as a “collection of things” which can be bought or provided. The “basic human needs approach” ignored the necessity for solvent markets to support increased agricultural yields and the populations that were being displaced. As they could no longer rely on agricultural work to sustain themselves, displaced workers failed to find jobs in nearby city economies, where labor markets had not evolved alongside the increased agricultural yields through a succession of appropriate feedback mechanisms triggering the needed corrections. And she made the same argument against technology transfers in the “Green Revolution” of the 1960s and 1970s. The mechanism of feedback relationships is one example among others of Jacobs’ usage of systemic concepts to draw boundaries around the city economy as a system and elaborate on its behavior. Further examination of Jacobs’ use of these concepts within the paradigm she adopted may reveal a consistent link between her analysis of cities as economic units and the policies she is tended to critique. In short, future attempts at more comprehensive interpretations of Jacobs’ economic thought might benefit from stepping away from the urban focus of The Death and Life of Great American Cities while considering more carefully her later economic writings. Divry on Jacobs 15.3 Regional Economics Rebours The history of regional science offers an interesting case study, as well as a one of the few examples, of the institutionalization of an entirely new scientific field in the years after 1945. Its foundation by Walter Isard and a group of social scientists in the 1950s represents the most institutionalized attempt to stimulate the relationship between economics and geography. The original project of Isard, who was trained as an economist at Harvard, was to promote the study of location and regional problems. And at the outset, regional science was, in various ways, a success. It attracted many scholars from different disciplines, mostly economics, geography and urban/regional planning, and it quickly became institutionalized formally through the foundation of the Regional Science Association (RSA) in 1954 and establishment of a Regional Science Department at the University of Pennsylvania in 1958. At the same time, the creation of the Papers and Proceedings of The Regional Science Association in 1955 and of the Journal of Regional Science in 1958, offered new publication venues for scholars interested in location analysis, in particular quantitative geographers who found it difficult to publish in traditional geography journals. Within economics, regional science influenced analytical works in urban economics, as, for instance, William Alonso’s thesis, widely recognized as one of the foundational works of urban economics, was written at Penn under the supervision of Isard in 1960. However, the prevailing processes of knowledge production and evaluation which shaped the emergence of this new field were deeply influenced by economics. Geographers became dissatisfied with Isard’s vision of the hierarchical division between geographers and economists, and the primacy given to economic theorizing and modelling as the core of the new regional science. Thus, the social organization of the field of regional science and its interactions with other disciplines mirrored the particularity of economics, a hierarchical discipline organized around a strong theoretical core and an insularity from the rest of social sciences. In the late 1940s, Isard became increasingly concerned about the lack of interest among economists in the location of economic activities. His perception of the subject was not really different to his colleagues, but he wanted to improve the theory they used, which, following the British tradition of the late 19th century, suffered from a lack of spatial dimension. He did not seek to challenge the general equilibrium economic theory that was becoming dominant, but sought instead to integrate a spatial aspect within it. In 1949 Isard was recruited to Harvard by Wassily Leontief to develop an input-output approach to regional development. During the war, input-output analysis received much attention because it enabled the American Air Force to identify the best targets for bombing. As a consequence, Leontief had received large research funds to develop his input-output framework. Isard expressed a hierarchical division between economists, who provided the analytical foundations of regional science, and the geographers, who provided the empirical facts and testing. While, the identity of economics was legitimated and reinforced by its success during the war, in geography, there was an increasing dissatisfaction with the regional geography approach that dominated the field in the1950s. The Cold War context facilitated the promotion of a new generation of quantitative geographers looking for more scientific methods. By the mid-1970s, regional science experienced a progressive decline when geographers started to distance themselves from the analytical methods that were promoted by Isard. But even after the Regional Science Department at Penn closed its doors in 1993, regional science journals remained a going concern and continued to promote studies of spatial issues notably from urban economics and, after 1991, New Economic Geography. Rebours "],["biophysical-economics.html", "16 Biophysical Economics", " 16 Biophysical Economics The field of ecological economics is extremely heterogeneous but can be separated, in my opinion, into two factions: those who wish to measure ecosystems in monetary units and those who wish to measure the human economy in biophysical units. (Blair Fix) Yan Abstract Global civilization is experiencing social and economic turmoil. Human are experiencing dete- rioration of environment and uncontrollable declines in GDP. Traditional economic theory has been continuously advancing yet seems unable to predict these crises or provide adequate public policies to address them. A biophysical version of economic theory uses mass and energy flows as well as environmental constraints to describe the delivery of goods and services. Ongoing development in biophysical economic theory may provide some new guidance. In this review paper, Authors analyze the progression of historical economic arguments, explore their assump- tions and their development and compare them to the currently developing biophysical econom- ics framework which, instead of focusing on investment, debt, and growth, focuses on sustainable energy and mass flows to deliver goods and services to civilization Yan Memo Table 3. Comparison between biophysical economics and mainstream economics. Content Neo-Classical Biophysical Wealth root Land, Labor, Capital Energy Distribution Market Resource Constraints Hypothesis Rational people Base on Nature Determinants Labor and Capital Energy Gov. role Macro Control Env.protection Yan (2019) (pdf) Fix This book tested four implicit assumptions made by neoclassical growth theory: Economic output can become decoupled from energy inputs. Economic distribution is unrelated to growth. Large institutions are not important for growth. Labor force structure is not important for growth. In all cases, the empirical evidence directly contradicted these assumptions. For those who think that a scientific theory should be based on empirically grounded facts, this critique alone provides compelling reasons to abandon neoclassical growth theory. To conclude, the shortcomings of neoclassical growth theory can be summarized as follows: It does not explain the phenomenon for which it is designed to explain. The majority of growth is attributed to the ‘Solow-residual’, which is an inter- nalized error function. Neoclassical economists model the residual with an exponential function of time. However, if resorting to a function of time, Oc- cam’s razor would suggest that we discard the remainder of the production function in favor of a pure function of time. There are fundamental problems associated with the measurement of the the- ory’s basic variables (output and capital input). The accepted method is to measure capital and output quantities by way of monetary value. However, such an approach requires making inherently subjective decisions, since the underlying unit (price) is not well-defined. Moreover, it appears that the cur- rent approach to measuring capital and output may be circularly dependent on neoclassical theory. Therefore, such metrics are inappropriate for testing neoclassic theory. Its implicit assumptions are directly contradicted by empirical evidence. Rather than being ‘innocuously’ untrue, the implicit assumptions made by neoclassical theory are ‘insidiously’ untrue. The theory excludes from its scope some of the most fundamental aspects of growth. Thus neoclassical growth theory maintains simplicity by courting irrelevance. The remaining empirical support for the theory is tautological. The strong empirical results on which neoclassical growth theory purportedly rests nei- ther elucidate the underlying technical form of the economy nor provide sup- port for the marginal productivity theory of distribution. Instead, they are the result of a tautological relation between the production function form and an algebraic transformation of the national accounts identity. Where, then, does this leave neoclassical growth theory? It seems fair to conclude that it is an elegant mathematical construct that has little to do with the real world. A Biophysical Approach Such a theory must begin by asking a very simple question, but one that is not often asked in economic theory: why do we have growth at all? Indeed, growth is such an ephemeral phenomenon in the history of humanity that its very existence should be surprising. In my opinion, satisfying theories about the origins of growth do not come from economics, but from thermodynamics and the study of complex systems. In order to understand why growth exists, I propose that we need only two hypotheses: All complex, non-equilibrium systems must be sustained by flows of energy and/or matter. Increases in these flows allow the system to expand. An industrial economy is a non-equilibrium system that is energetically sus- tained primarily by exploitation of the finite stock of fossil fuels. Growth, then, is possible whenever a new energy source is made available. Prior to industrialization, technological constraints prevented humans from exploiting fossil fuel energy. However, once sufficient technology existed, a feedback-loop set in. Previously harvested resources and energy were transformed into technology that was powered by fossil fuels and which generated enough surplus to not only power the economy but to exploit further fuels. Continuous iteration of this loop led to exponential growth. Biophysical growth, as I have defined it in this book, is the increase in the rate at which resources (specifically energy) flow through the economy. Thus, in Bardi and Lavacchi’s model, biophysical growth is represented by the rate of resource extraction ( Ṙ). A robust feature of this model is that it produces bell-shaped resource extraction curves through time. This model gives some analytic rigor to the peak and decline scenario envisioned at the outset of the book. But while it indicates that a future energy consumption curve might be bell-shaped, it does not indicate how a future energy decline will affect society. It is also important to distinguish between external and internal con- straints to growth. External (resource) constraints can describe the long-run behavior of the economy, but internal (social) constraints dominate the short-run. Historical crises have almost all been due to internal, social dynamics. Even with the imposition of external, biophysical constraints, there is little reason to think that complex social dynamics will cease to be of importance in the future. Thus, an understanding of the future will require models, but also in-depth empirical study of the past. A Biophysical Approach Given the inadequacy of neoclassical theory, what is the best alternative? As should be obvious by now, I think that a biophysical approach to growth theory provides the most suitable way forward. Such a theory must begin by asking a very simple question, but one that is not often asked in economic theory: why do we have growth at all? Indeed, growth is such an ephemeral phenomenon in the history of humanity that its very existence should be surprising. In my opinion, satisfying theories about the origins of growth do not come from economics, but from thermodynamics and the study of complex systems. In order to understand why growth exists, I propose that we need only two hypotheses: All complex, non-equilibrium systems must be sustained by flows of energy and/or matter. Increases in these flows allow the system to expand. An industrial economy is a non-equilibrium system that is energetically sus- tained primarily by exploitation of the finite stock of fossil fuels. Growth, then, is possible whenever a new energy source is made available. Prior to industrialization, technological constraints prevented humans from exploiting fossil fuel energy. However, once sufficient technology existed, a feedback-loop set in. Previously harvested resources and energy were transformed into technology that was powered by fossil fuels and which generated enough surplus to not only power the economy but to exploit further fuels. Continuous iteration of this loop led to exponential growth. What is the simplest way to model this feedback-loop? Bardi and Lavacchi (2009) have shown that the famous Lotka-Volterra equations (which are usually used to model predator-prey dynamics) can be adapted to model the resource ex- ploitation process: \\[ Ṙ = −k_1 T R\\] \\[Ṫ = k_2 T R − k_3 T\\] Here \\(R\\) represents a resource stock and \\(T\\) represents a stock of technological in- frastructure. Equation 1 states that the rate at which the resource is harvested ( \\(Ṙ\\) ) depends upon the size of the technological stock, the size of the resource stock, and the efficiency of resource extraction (\\(k_1\\) ). This equation indicates that a greater technological stock can accelerate resource exploitation, but as the size of the resource stock dwindles (as R decreases), the pace of resource exploitation will slow. Equation 2 states that harvested resources are transformed into technology. The rate of this transformation (\\(Ṫ\\) ) is dictated by the rate of resource harvest (T R) and the efficiency of the transformation process (\\(k_2\\) ). Additionally, technology (and its instruments) is subject to entropic decay (\\(−k_3 T\\) ) at a rate determined by \\(k_3\\) . Biophysical growth, as I have defined it in this book, is the increase in the rate at which resources (specifically energy) flow through the economy. Thus, in Bardi and Lavacchi’s model, biophysical growth is represented by the rate of resource extraction ( \\(Ṙ\\)). A robust feature of this model is that it produces bell-shaped resource extraction curves through time. Thus, the essential insights of this model are: growth can be modelled in terms of a feedback-loop between technology and natural resource extraction; and the ultimate growth limit is set by the size of the finite stock of resources. This model gives some analytic rigor to the peak and decline scenario envisioned at the outset of the book. But while it indicates that a future energy consumption curve might be bell-shaped, it does not indicate how a future energy decline will affect society. It is also important to distinguish between external and internal con- straints to growth. External (resource) constraints can describe the long-run behavior of the economy, but internal (social) constraints dominate the short-run. Historical crises have almost all been due to internal, social dynamics (think of the Great De- pression). Even with the imposition of external, biophysical constraints, there is little reason to think that complex social dynamics will cease to be of importance in the future. Thus, an understanding of the future will require models, but also in-depth empirical study of the past. What is needed is a biophysical research agenda – one that seeks to systematically understand the relation between energy consumption and all aspects of human society. Energy scholars such as Ayres and Warr (2009), Giampietro et al. (2012), Hall and Klitgaard (2012) and Smil (2010) have made significant contributions on this front, but much more work is needed. Stylized Biophysical Facts As I stated at the outset of the book, a good starting point for a new theory is to investigate the assumptions made by existing theory. If the results of this book tell us nothing else, it is that a good starting place for a biophysical growth theory is to begin with what neoclassical theory ignores. Neoclassical growth theory ignores the role of energy, yet the expansion of energy consumption is the single most im- portant aspect of growth. distribution, yet distribution is fundamentally connected to growth. large institutions, yet such institutions play a central role in growth. changes in labor structure, yet changes in this structure are essential to growth. A theory is always the product of the phenomena it seeks to explain. What does neoclassical growth theory seek to explain? Nearly 60 years ago, Nicholas Kaldor (1957) outlined six statements that came to be known as the ‘Kaldor facts’ of eco- nomic growth. In many ways, the goal of neoclassical growth theory has been to explain these facts. Kaldor’s facts can be paraphrased as follows: Output per worker grows at a roughly constant rate that does not de- crease over time. Capital per worker increases over time. The capital/output ratio is roughly constant. The rate of return to capital is roughly constant. The share of capital and labor in net income are roughly constant. Labor productivity growth rates vary considerably between societies. Notice that 5 out of 6 of these facts are concerned with either something that re- mains ‘constant’ (facts 1, 3, 4, 5) and/or something that ‘grows over time’ (facts 1, 2). The logical offspring of these facts is a theory in which growth is constant and inevitable (i.e. neoclassical growth theory). Notice also the focus on capital. Neo- classical growth theory places capital at the center of its explanation of growth, but never bothers to explain where capital comes from. This neglect is likely the result of the neoclassical duality of capital. Note that when one applies compound interest to financial capital, the financial stock will grow expo- nentially. Neoclassical theory takes the logic of financial capital and applies it to the physical capital stock. Yet such a stock cannot be self-perpetuating – the laws of thermodynamics forbid it. Neoclassical theory fails to see that physical capital (i.e. a technological stock) is primarily a means for converting energy into useful work. Without an energy flow, physical capital cannot fulfil its purpose (think of a tractor without fuel). By focusing on constant and inevitable growth driven by the accumulation of capital, neoclassical theory set itself on the wrong course from the very beginning. The focus of a growth theory should be on energy. Energy is the driving force that sustains all biophysical systems. Seven Stylized Biophysical Facts Trends accompanying increases in energy use per capita: Large institutions (corporations and governments) increase their employment share. Agricultural employment decreases. Service employment increases. Trends accompanying increases in the energy use per capita growth rate: The value of production increases relative to the price of energy. The share of profit in national income increases. Debt claims decrease relative to the value of production. Downward income redistribution is more likely to occur. A good starting point for a biophysical growth theory is to attempt to explain these seven stylized biophysical facts in a way that is both internally coherent and consilient with accepted scientific knowledge. Energy is the “universal currency”. Fix (2015) Biophysical Growth Theory (pdf) ** Hall and Klitgaard Preface* There are four books on our shelf that have the words, more or less, “wealth of nations” in their titles. They are Adam Smith’s 1776 pioneering work, An Inquiry into the Nature and Causes of the Wealth of Nations, and three of recent vintage, David Landes’ The Wealth and Poverty of Nations, David Warsh’s Knowledge and the Wealth of Nations, and Eric Beinhocker’s The Origin of Wealth. Warsh’s book is rather supportive of current approaches to economics while Beinhocker’s is critical, but all of these titles attempt to explain, in various ways, the origin of wealth and propose how it might be increased. Curiously, none have the word “energy” or “oil” in their glossary (one trivial exception), and none even have the words “natural resources.” How can someone write a book about economics without mention- ing energy? How can economists ignore what might be the most important issue in economics? Within the discipline of economics, economic activity is seemingly exempt from the need for energy and matter to make economies happen, as well as the second law of ther- modynamics. Instead we hear of “substitutes” and “tech- nological innovation,” as if there were indefinite substitutes for matter, energy, and the environment. Why is economics construed and taught only as a social science, since in reality economies are as much, and per- haps even principally, about the transfor- mation and movement of all manner of biophysical stuff in a world governed by physical laws? Part of the answer lies in the recent era of cheap and seemingly limitless fossil energy which has allowed a large proportion of humans to basically ignore the biophysical world. Without significant energy or other resource constraints, economists have believed the rate-determining step in any economic transaction to be the choice of insatiable humans attempting to get maxi- mum psychological satisfaction from the money at their disposal, and markets seemed to have an infinite capacity to serve these needs and wants. Indeed the abundance of cheap energy has allowed essentially any economic theory to “work” and economic growth to be a way of life. For the last century, all we had to do was to pump more and more oil out of the ground. However, as we enter a new era of “the end of cheap oil,” energy has become a game changer for economics and anyone trying to balance a budget. In brief, this book: 5 Provides a fresh perspective on eco- nomics for those wondering “what’s next” after the crash of 2008 and the near cessation of economic growth for much of the Western world since then 5 Summarizes the most important infor- mation needed to understand energy and our potential energy futures In summary, this is an economics text like no other, and it introduces ideas that are extremely powerful and are likely to trans- form how you look at economics and your own life. Hall and Klitgaard (2018) Energy and the Wealth of Nations (pdf) Ayres Conclusion The first conclusion from the above analysis is that growth in exergy consumption generally, and electric power consumption in particular, have had an enormous impact on past economic growth. The mechanism responsible has recently been dubbed the rebound effect’ which conveys the notion that increasing efficiency tends to result in lower costs, which trigger increasing demand that (often) results in greater – rather than less – exergy consumption. The second conclusion from our analysis is that thermodynamic efficiency improvements in the production of primary work can account for most of the so- calledSolow residual’, namely that portion of economic growth attributable to `technical progress.’ Secondary work (end-use efficiency improvements) in transportation and some uses of electric power e.g. for lighting) may account for a considerable part of the remainder. We conjecture that the unexplained part of the Solow residual (since 1980) may be mostly attributable to the impact of information technology. The third important conclusion is that, technical progress in the past notwithstanding, there is still an enormous potential for future reductions in exergy consumption, especially in the residential and commercial heating area. A fourth and final conclusion of this paper is that the locus of technical progress has moved from energy (exergy) conversion efficiency to end-use efficiency or service output per unit of work (SOPUW). Purely thermodynamic efficiency improvements were largely exhausted by the 1960s. This does not rule out the possibility of further thermodynamic improvements in the future. However most gains since then have arisen from other factors. Although we have not attempted a detailed accounting of the latter category of improvements, it is very plausible that reduced material consumption per unit of service output has been a major driver of these gains, and that information technology will make increasingly important contributions in the future. A subtler but related, and arguably more important, question is whether the rebound effect is still the primary driver of economic growth and to what extent growth can be expected if the consumption of fossil fuels – the major source of primary exergy in the modern world – can be curtailed in order to stabilize the climate and minimize other kinds of environmental damage. Ayres (2003) Energy,Power and Work in the US Economy (pdf) "],["ecological-economics.html", "17 Ecological Economics 17.1 Natural Resources and Energy 17.2 Against Steady-State Economics", " 17 Ecological Economics Its challenge would not be to blend the different domains of study under the same mindset – combining plants and profits in a single analysis – but to train students to see in complementary, but conflicting, ways. While ‘environmental economists’ argue that we can easily correct markets by pricing carbon emissions and other pollutants – no matter that we barely have, in practice – the larger issue is that many of our ecological challenges are not amenable to a commodification ‘fix’, which relies on treating the environment as parts. The issue comes to a head in the question of whether we should impute dollar values for ‘ecosystem services’ – to put a price on the Amazon rainforest, say. The question is not whether we can impute such values, but rather whether it is intelligent to do so. In this critical matter, which has divided ecologists, is the issue of whether ecology should yield to a dominant economic way of thinking or make a stand for its different way of seeing – a different way of appreciating and valuing – that challenges economics’ monetary default. The pragmatic view has been to impute monetary values because we cannot afford for ecosystems to be valued at zero. [DH: rather - at infinite! nature is Holy!!] Indeed, when such estimates are made, they reveal that the ‘value’ of global ecosystem services dwarfs global GDP! Market measures of value miss more than they grasp. Sustainable business is confronting the fact it does not constitute ‘ecological’ thinking but rather the appropriation of some ecological concerns into a framework that remains steadfastly economic. The scale and stubbornness of major problems simply may not yield to a more-of-the-same technological fixing mentality, but instead require deeper cultural rebalancing, through a ground-up awakening. The broader vision of the ecologist has room to understand the role the economist plays, but the economist – and the businessman and the investor – do not seem to know that they need the ecologist. Austin (2021) The Matrix of the Emissary - Market Primacy and The Sustainability Crisis Parrique Let me introduce Romanian-American mathematician and economist Nicholas Georgescu-Roegen (1906–1994) who, at the beginning of the 1970s, laid out one theory so disruptive that it led to the creation of a new school of economic thought: ecological economics. His main idea, exposed in The Entropy Law and the Economic Process (1971), was that economic organization is a continuation of biological organization. Why? Because all machines are necessarily made of materials and use energy, and because all labour involves our biological bodies, which are also made of materials and use energy. The economy is — unavoidably — a bioeconomy, which means it is a subsystem of the larger finite and non-growing ecosystem that is the Earth. The logical conclusion becomes inevitable: nature holds non-negotiable market power and humans can only use whatever nature supplies. This also means that the prosperity of the economy is fundamentally linked to that of ecology. In the same way that a healthy organ cannot thrive for long in a dying body, an economy will not prosper within a collapsing biosphere (or at least not for long). In terms of manufacturing, this means that certain factors of production are non-substitutable. Any human-made artefact is necessarily made out of natural resources such as materials and energy and so therefore cannot be a true substitute to it. “One cannot build the same wooden house with half the timber no matter how many saws and carpenters one tries to substitute,” wrote Herman Daly (another economist who has laid out a deep theory to explain why infinite growth is an ecological impossibility). Regardless of how ingenious you are and the budget of your R&amp;D department, you will not be able to build a wooden house without wood. If all economic activities require energy and materials, it means economic practices are unavoidably entropic (the second law of thermodynamics), which means they neither create nor destroy matter or energy but only transform it from a higher to a lowerquality. Consider this an inescapable law of diminishing returns applied to the economy as a whole. You can produce more for a time, and produce more efficiently to be able to keep producing for a longer period of time, but you cannot keep increasing production forever. This is because all of the materials and energy we use come from a nature that is fundamentally finite in its ability to provide resources and assimilate waste. What kind of theory do green growth advocates offer in opposition to that? Well, not much, in my opinion. The core assumption of modern mainstream economics comes from a 1974 paper from American economist Robert Solow where he integrated natural resources as an input into the neoclassical production function while assuming its perfect substitutability with human-made capital. “If it is very easy to substitute other factors for natural resources,” Solow writes, “the world can, in effect, get along without natural resources.” Now, economists who think this makes sense should spend a bit more time in their garden, realising that it is not “very easy” (or even possible at all) to substitute other factors for natural resources (good luck growing food with a high-tech, smart shovel but without soil, bees, and water). So now, which theory should we choose? Should we trust experts who have developed their entire school of economics since the 1980s on the very question of how economy interacts with ecology, or should we rather ask a random neoclassical economist what they think on a matter they have only studied peripherally? I love both Nicholas Georgescu-Roegen and Robert Solow for different reasons, but picking Solow to understand the relation between growth and the environment would be like picking Zlatan Ibrahimović to play tennis — not the wisest pick. The current hype for green growth is scientifically ungrounded, both empirically and theoretically. Parrique (2022) Degrwoth is Good Economics Parrique Home (pdf) 17.1 Natural Resources and Energy Garzon The economist Georgescu-Rogen (2007) was one of the first to warn of the serious deficiencies in traditional ways of thinking about the economy. In particular, he highlighted the gap in economic models regarding the consumption of energy and materials. Both components restrict the possibilities of economic growth in ways that economics had ignored until just a few years ago[1]. In fact, planet Earth is a closed system of materials so that, aside from the very exceptional arrival of a meteorite or the removal of a human artefact, neither of which are significant in quantitative terms, the mass of materials is always the same. In the case of energy, planet Earth is an open system inasmuch as we receive energy flows from solar radiation, but even then, the laws of physics impose limits on energy use. Every human process involves use of a series of energy sources governed by the laws of physics, particularly the laws of thermodynamics. The second principle of thermodynamics establishes that the quality of energy usable by human beings is decreasing and that, in converting energy (for example, converting the energy deriving from solar radiation to photosynthesis or generating electricity through photovoltaic panels), it is not possible to maintain 100% of the available energy. Much of the energy is dissipated as heat, so that conversion presupposes the transformation of high-quality, low-entropy energy, such as carbon, into low-quality, high-entropy energy such as heat. The history of technological development is the history of a constant struggle to improve the energy efficiency of such conversions. Flows of materials and flows of energy can be understood as two distinct aspects of the same process. In fact, a continuous flow of materials is only possible if there is a continuous flow of energy at the same time. In addition, these two restrictions on economic growth interact in very diverse ways and the ecological pressure and impact of productive activity also show up in the alteration of geochemical cycles. There is no doubt that human beings have lived on Earth for at least two hundred thousand years, although most of the time they did so in hunter-gatherer social groups. The end of the last ice age, which occurred some twenty thousand years ago, gave way to an extraordinarily warm climate which, in its turn, enabled human beings to develop new economic and social practices, such as agriculture (developed some 12,000 years ago). Scientists have agreed to call this warm era the Holocene, in which current civilizations developed. Planetary Boundaries and Eco-Social Crisis One of the main problems with the planetary boundaries’ framework, however, is that it looks at social metabolism in an essentially technical way. If the analysis is not broadened, the framework seems to place responsibility on abstract notions such as «humanity» or «the human being», when it is obvious that neither the causes nor the consequences of the ecological impact are symmetrically distributed either across the class structure or between the different geographical regions. There is in fact no global ecological crisis which means the same for all human beings (Brand et al., 2021). Therefore it is much more appropriate to talk of an eco-social crisis, because this helps to highlight the importance of socio-political relationships when assessing environmental degradation processes and seeking solutions. Garzon (2022) The limits to growth: eco-socialism or barbarism 17.2 Against Steady-State Economics Vettese Underlying Herman Daly’s ecological economics is a faith in markets, neo-liberal regulatory tools and theory, and Malthusianism. While Daly criticizes economic growth, he overestimates the ability of regulation to contain a capitalist economy within a ‘steady-state’. Cap-and-trade is his main tool to regulate a steady-state economy, even though that tool emerged from neo-liberal thought and has been instrumental in stymying the environmental movement’s progress. Moreover, the neo-liberal Julian Simon developed a powerful critique of environmentalism in the 1980s, which Daly has not responded to. Over the last half-century, neo-liberal environmental thought has cast a shadow over ecological economics, even though Daly seems unable to perceive its influence on his life’s work. If the environmental movement wants to win the fight, then it needs an entirely new ecological economics. Vettese (2020) Against Steady State Economics (pdf) "],["econophysics.html", "18 Econophysics 18.1 Economy as dissipative system", " 18 Econophysics Jovanovic Although financial economics and mathematical finance still largely dominate modern financial theory, in the past few years a new player has increasingly been making itself felt, and could lead to a rethinking of some of the theoretical foundations of modern financial theory. This new player is econophysics. This article makes three contributions to the history of modern financial theory: an analysis of the theoretical foundations of econophysics (and their connections with the history of financial economics); a study of the reasons underlying the emergence of econophysics; and a presentation of the manner in which econophysics has become the third component of modern financial theory. Econophysics’ major distinguishing feature is the use of pure Lévy processes. Jovanovic (2013) History of Econophysics’ Emergence (pdf) Blair Fix Econophysics is an attempt to understand economic phenomena (like the distribution of income) using the tools of statistical mechanics. The particle model of physics demonstrates how a seemingly equal process (the random exchange of energy) can give rise to wide inequalities. If econophycisists are correct, this model tells us why human societies are mired by inequality. It’s just basic thermodynamics. The idea required a leap of faith: treat humans like gas particles. Econophysicists highlighted an interesting parallel. When humans exchange money, it is similar to when gas particles exchange energy. One party leaves with more money/energy, the other party leaves with less. With the parallel between energy and money, ecophysicists arrived at a startling conclusion. Their models showed that when humans exchange money, inequality is inevitable. When econophysicists use ‘random exchange’ to explain income, many people are horrified by the lack of causality. To understand the behavior of large groups of particles, Boltzmann was forced to use the mathematics of probability. The resulting uncertainty in cause and effect made him uneasy. Quantum mechanics would later show that at the deepest level, nature is uncertain. But this quantum surprise does not mean that probability and determinism are always incompatible. In many cases, the use of probability is just a ‘hack’. It is a way to simplify a deterministic system that is otherwise too difficult to model. Like a coin toss, econophysicists think we can treat monetary exchange in probabilistic terms. Econophysicists think we can model the exchange of money without understanding property transactions. Blair Fix Garrett Abstract Climate change is a two-way street during the Anthropocene: civilization depends upon a favorable climate at the same time that it modiﬁes it. Yet studies that forecast economic growth employ fundamentally diﬀerent equations and assumptions than those used to model Earth’s physical, chemical, and biological processes. In the interest of establishing a common theoretical framework, this article treats humanity like any other physical process; that is, as an open, nonequilibrium thermodynamic system that sustains existing circulations and furthers its material growth through the consumption and dissipation of energy. The link of physical to economic quantities comes from a prior result that establishes a ﬁxed rela- tionship between rates of global energy consumption and a historical accumulation of global economic wealth. What follows are nonequilibrium prognostic expressions for how wealth, energy consumption, and the Gross World Product (GWP) grow with time. This paper shows that the key components that determine whether civilization “innovates” itself toward faster economic growth include energy reserve discovery, improvements to human and infrastructure longevity, and reductions in the amount of energy required to extract raw materials. Growth slows due to a combination of prior growth, energy reserve depletion, and a “fraying” of civilization networks due to natural disasters. Theoretical and numerical argu- ments suggest that when growth rates approach zero, civilization becomes fragile to such externalities as natural disasters, and is at risk is for an accelerating collapse. Linking physical to economic quantities comes from a ﬁxed relationship between rates of global energy consumption and historical accumulation of global economic wealth. When growth rates approach zero, civilization becomes fragile to externalities, such as natural disasters, and is at risk for accel- erating collapse. Garrett Memo As with any other natural system, civilization is composed of matter. Internal circulations are maintained by a dissipation of potential energy. Oil, coal, and other fuels “heat” civilization to raise the potential of its internal components. Dissipative frictional, resistive, radiative, and viscous forces return the potential of civilization to its initial state, ready for the next cycle of energy consumption. The material growth and decay of civilization networks is driven by a long-run imbalance between energy consumption and dissipation. Treating civilization as a dissipative physical system like any other on our planet. Garrett Summary This paper has presented a physical basis for interpreting and forecasting global civilization growth, with the intent that it might be used to develop a consistent theoretical basis for forecasting interactions between humanity and climate during the Anthropocene. The perspective is that, like a living organism [Vermeij, 2009], energy consumption and dissipation drives material ﬂows to civilization. If there is a net convergence of matter within civilization, then civilization grows. Growth increases the availability of new and existing reserves of matter and energy, and this leads to a positive feedback loop that allows growth to persist or even accelerate. These rather general thermodynamic results can be expressed in purely economic terms because there appears to be a ﬁxed link between global rates of primary energy consumption and a very general expres- sion of human wealth: 𝜆 = 7.1 ± 0.1 Watts of primary energy consumption is required to sustain each $1000 of civilization value, adjusting for inﬂation to the year 2005 (see supporting information and Garrett [2012a]). It was argued that wealth does not rest in inert “physical capital”, as in traditional treatments. Rather, wealth can be interpreted to include all aspects of civilization, even the purely social. Value lies in the density of a network of connections between civilization elements, insofar as this network contributes to a global scale consumption and dissipation of energy (equation (41)). Global economic production Y is positive when consumption exceeds dissipation, and there is a net diﬀusion of matter to civilization that grows its size. This leads to an economic growth model for wealth C and economic production Y that is more simple, physical, and dimensionally self-consistent than mainstream models: dC = Y dt (70) Y = 𝜂C (71) where Y is directly proportional to a lengthening of civilization’s networks and growth of its energy reserves. The real rate of return on wealth 𝜂 is somewhat analogous to the total factor productivity in traditional models. Prognostic expressions for 𝜂 presented here show that its value is determined by a combination of rates of civilization decay, the quantity of available energy reserves, the amount of energy required to incorporate raw materials into civilization’s structure, and the accumulated size of civilization due to past raw material ﬂux convergence. Current values of the rate of return can be inferred from equation (71). For example, current global rates of return are about 2.2% per year [Garrett, 2012a]. Trends in 𝜂 can be forecast based on estimates of future decay and rates of raw material and energy reserve discovery (equation (56)). Thus, this paper oﬀers a set of prognostic expressions for the growth of civilization, expressible in economic and energetic terms that can be linked to physically measurable quantities. The implications that have been described are summarized as follows: - Civilization inﬂation-adjusted wealth is sustained by global energy consumption and grows only as fast. - Some combination of price inﬂation and unemployment is related to rates of civilization decay. - Rates of return on wealth decline in response to accelerated decay or increased resource scarcity. - Rapid rates of current growth act as a drag on future rates of growth. - Rates of return grow when there is “innovation” through technological change. - The GWP grows when energy consumption grows super-exponentially (at an accelerating rate), or when global energy reserve discovery exceeds depletion. - If growth rates of wealth approach zero, civilization becomes fragile with respect to externally forced decay. This appears to be particularly true if prior growth was super-exponential. Many of these conclusions might seem intuitive, or as if they have been expressed already by others within more traditional economic perspectives. What is novel in this study is the expression of the eco- nomic system within a deterministic thermodynamic framework where a very wide variety of economic behaviors are derived from only a bare minimum of ﬁrst principles. More importantly, a suﬃcient set of statistics exists for global economic productivity, inﬂation, energy consumption, raw material extraction and energy reserve discovery that the nonequilibrium solutions presented here can be evaluated and falsiﬁed with no requirement for any a priori tuning or ﬁtting to historical data. Such evaluation will be addressed in Part II. Speciﬁcally, it will be shown that the logistic equation given by equation (64) closely matches the evolution of global economic rates of return since 1950, allowing for observed rates of technological change deﬁned by equation (56). Logistic behavior has been recognized in the evolution of human empires throughout history. It will be shown to be evident in global rates of economic growth as well. Global civilization has enjoyed explosive growth since the industrial revolution, but it is unclear how long this can be sustained when it is facing ongoing resource depletion, pollution, and climate change. Global economic wealth is tied to energy consumption, and energy consumption through combustion is tied to carbon dioxide emissions. Without a suﬃciently rapid switch to noncarbon sources of energy, growing wealth is necessarily linked to growing emissions. Yet accumulating carbon dioxide in the atmosphere is also likely to drive accelerating civilization decay through ampliﬁed hydrological extremes, storm intensiﬁcation, sea level rise, and mammalian heat stress. The prognostic expressions that have been derived here might be useful to help guide a physically plausible range of future timelines for civilization growth and decay, particularly in models that couple human and climate systems during the Anthropocene. Garrett (2014) Long-run evolution of the global economy:1. Physical basis (pdf) 18.1 Economy as dissipative system Ayres In a closed Walrasian model resources are assumed t o be generated by labor and capital. The neo-classical (Walrasian) equilibrium system does not qualify as a dissipative structure. The neoclassical system is, in effect, a per- petual motion machine. This fact was emphatically pointed out by the Nobel prize-winning chemist F. Soddy in 1922 (Daly, 1980), but Soddy’s work was vir- tually ignored by economists. The first economists to stress the dissipative nature of the economic system were Boulding (1966) and Georgescu-Roegen (1971). The relevance of mass and energy conservation to environmental- resource economics was first emphasized by Kneese et al. (1970). In reality, resource inputs originate outside the economic system per se: they include air, water, sunlight and material substances, fuels, food, and fiber crops, all of which embody free energy or available work. The economic system, in reality, is absolutely dependent on a continu- ing flow of free energy from the environment. Evidently, the real economic system looks very much like a self-organizing dissipative structure in Prigogine’s sense: it is dependent on a continuous flow of free energy (the sun or fossil fuels), and it exhibits coherent, orderly behavior. Moreover, like living organisms, it embodies structural information as morpho- logical differentiation and functional specialization. Since the economy is, by assumption, a dissipative structure, it depends on a continuous flow of free energy and materials from and to the environ- ment. Such links are precluded by closed neoclassical general equilibrium models, either static or quasi-static. The energy and physical materials inputs to the economy have shifted over the past two centuries from mainly renewable to mainly nonrenewable sources. Dynamic economic growth is driven by technological change (generated, in turn, by economic forces), which also results in continuous structural change in the economic system. For instance, so-called Leontief input- output coefficients do not remain constant. It follows, incidentally, that a long-term survival path must sooner or later reverse the historical shift away from renewable resources. This will only be feasible if human technological capabilities continue to rise to levels much higher than current ones [8]. But, since technological capability is itself an output of the economic system, it will continue to increase if, and only if, deliberate investment in R&amp;D is continued or even increased. In short, the role of knowledge-generating activity in retarding global entropy seems to be growing in importance. The economic system is not necessarily stable against all pertur- bations, and the more it is intentionally managed to optimize growth, the more it becomes vulnerable to the consequences of human error. Ayres (1988) Self-organisation in Biology and Economics (pdf) Shiozawa Many protests and contestations have been voiced out against equilibrium theory. Some argued that it neglects the increasing returns to scale which underlies in the development of modern industries. Others contested the maximizing principle which is always supposed in the formulation of economic behaviors, both for consumers and producers. In 1970’s, many eminent economists criticized the state of the art of economic science and proposed to abandon a equilibrium analysis. But, this has not been done, partly for lack of new framework and partly for fear of us loosing ready made formulae for economic behaviors. New image of systems theory is requested and I think this new image should be the notion of “dissipative structure”. Professor Prigogine, in his early days of his research, was interested in non-equilibrium phenomena and remarked to the dissipative structure, which appears both in space and time. The importance of dissipative structure is evident, if one once knows that any living systems and subsystems are far from equilibrium but that they are all dissipative structure. Most simple example of dissipative structure is given as the flame of a candle. Once lid, a candle continues to burn unless all wax is consumed or the oxygen is exhausted. Dissipative structure sometimes takes the form of stationary state but it is very different from equilibrium. The latter is sensitive to boundary conditions. The concept of dissipative structure is important for economics, because it makes possible to have new idea how economic system works. In the equilibrium framework, boundary conditions are imposed as constraints of the system. In the dissipative framework, boundary conditions are not directly relected to the speed of the consumptions or the extent of employment. It is instead the internal structure which determines volumes and speeds of economic quantities. Most simple example is the extent of cultivated field. When there is a large surface of cultivable field and there is relatively small population, it is easy to see that whole surface is not necessarily cultivated. Some part which can be cultivated by the population will be cultivated effectively. Keynes was the first person to realize that, in economy, it is not the boundary condition or the amount of resources which determines how much of the resources are used. 50 years have passed after Keynes went to other world. During these years, many efforts had been made, in vain, to harmonize Keynesian macroeconomic theory with the neo-classical micro-economics. This is a natural outcome. The micro-economics, which is based on equilibrium framework, denies the existence of internal structure such as dissipative structure. Unless we are emancipated from the framework of general equilibrium, there will be no breakthrough for a new economics. If the problem is only the existence of internal structure, the economics system can be characterized as self-organizing system. But, the economy is not only a self-organizing system. Viewed as an ecological system, it is a system which constantly brings resources in and cast waste off. Economic activities are based on the constant flow of energy and materials. So the economy is also a dissipative structure. The proper difficulty of the economics is that the complexity is the real condition for the economic agents. This is not true for physical and chemical sciences. If we consider the boundednes of our rationality, it becomes rather evident that our behavior is not directed by a decision made once for all. It is a continuous sequence of adaptive adjustments, which will be organized according to rough program of purpose pursuit. Consequently, the theoretical framework of the economics should be reorganized as process analysis. Equilibrium analysis has been the obstruction for the economics to proceed to this old but still new direction. Shiozawa (1996) Economy as a Dissipative Structure (pdf) Caballero Some of the motivations for the econophysics literature do strike a chord with the task ahead for macroeconomists. For example, Albert and Barabási (2002), in advocating for the use of statistical mechanics tools for complex networks, write: Physics, a major beneficiary of reductionism, has developed an arsenal of successful tools for predicting the behavior of a system as a whole from the properties of its constituents. We now understand how magnetism emerges from the collective behavior of millions of spins . . . The success of these modeling efforts is based on the simplicity of the interactions between the elements: there is no ambiguity as to what interacts with what, and the interaction strength is uniquely determined by the physical distance. We are at a loss, however, to describe systems for which physical distance is irrelevant or for which there is ambiguity as to whether two components interact . . . there is an increasingly voiced need to move beyond reductionist approaches and try to understand the behavior of the system as a whole. Along this route, understanding the topology of the interactions between the components, i.e., networks, is unavoidable . . . The complex-systems literature itself offers fascinating examples of the power of interconnectedness. Bak, Chen, Scheinkman, and Woodford. (1992) and Sheinkman and Woodford (1994) bring methods and metaphors from statistical mechanics to macroeconomics. They argue that local, nonlinear interactions can allow small idiosyncratic shocks to generate large aggregate fluctuations, rather than washing out via the law of large numbers. They discuss a kind of macroeconomic instability called “self-organized criticality,” comparing the economy to a sand hill: at first, a tiny grain of sand dropped on the hill causes no aggregate effect, but as the slope of the hill increases, eventually one grain of sand can be sufficient to cause an avalanche. In the limit, aggregate fluctuations may emerge from hard-to-detect and purely idiosyncratic shocks. Put differently, a complex environment has an enormous potential to generate truly confusing surprises. This fact of life needs to be made an integral part of macroeconomic modeling and policymaking. Reality is immensely more complex than models, with millions of potential weak links. After a crisis has occurred, it is relatively easy to highlight the link that blew up, but before the crisis, it is a different matter. All market participants and policymakers know their own local world, but understanding all the possible linkages across these different worlds is too complex. The extent to which the lack of understanding of the full network matters to economic agents varies over the cycle. The importance of this lack of understanding is at its most extreme level during financial crises, when seemingly irrelevant and distant linkages are perceived to be relevant. Moreover, this change in paradigm, from irrelevant to critical linkages, can trigger massive uncertainty, which can unleash destructive flights to quality. Mandelbrot (2008, in a PBS NewsHour interview with Paul Solman on October 21, 2008) said: “[T]he basis of weather forecasting is looking from a satellite and seeing a storm coming, but not predicting that the storm will form. The behavior of economic phenomena is far more complicated than the behavior of liquids or gases.” When acute financial distress emerges in parts of the financial network, it is not enough to be informed about these direct trading partners, but it also becomes important for the banks to learn about the health of the partners of their trading partners to assess the chances of an indirect hit. As conditions continue to deteriorate, banks must learn about the health of the trading partners of the trading partners of their trading partners, and so on. At some point, the cost of information gathering becomes too large and the banks, now facing enormous uncertainty, choose to withdraw from loan commitments and illiquid positions. A flight-to-quality ensues, and the financial crisis spreads. The common aspects of investor behavior across these episodes―re-evaluation of models, conservatism, and disengagement from risky activities―indicate that these episodes involved Knightian uncertainty and not merely an increase in risk exposure. The extreme emphasis on tail outcomes and worst- case scenarios in agents’ decision rules suggests aversion to this kind of uncertainty. …conflated the possibility of catastrophe with catastrophe itself. The very acceptance of the key role played by complexity in significant macroeconomic events should be enough to point us in the direction of the kind of policies that can help to limit macroeconomic turbulence. Caballero (2010) Macroeconomics after the Crisis: Time to Deal with the Pretense-of-Knowledge Syndrome Durlauf Abstract This article explores the state of interplay between recent efforts to introduce complex systems methods into economics and the understanding of empirical phenomena. The empirical side of economic complexity may be divided into three general branches: historical studies, the iden- tification of power and scaling laws, and analyses of social interactions. I argue that, while providing useful ‘stylised facts’, none of these empirical approaches has produced compelling evidence that economic contexts exhibit the substantive microstructure or properties of com- plex systems. This failure reflects inadequate attention to identification problems. Identification analysis should therefore be at the centre of future work on the empirics of complexity. Durlauf Memo There are three main areas of work on the complexity/empirics interface. The first consists of historical studies. The study of economic complexity was in fact originally championed to a large extent by economic historians in the context of empirical studies of path dependence in economic activity. The second consists of the identification of data patterns that are consistent with some of the features of complex environments. A major feature of this work has been the effort to identify where power laws, which represent a particular class of probability distributions, and scaling laws, which describe relationships between variables that appear to be independent of the scale of measurement, occur in various economic data series. This search has to a substantial extent been led by physicists as there are a number of physical systems in which such laws are present. A third area of work has focused on the study of social interactions. To a large extent, this work has eschewed an explicit connection to complexity; nevertheless a number of social interactions models, e.g. Brock and Durlauf (2001a,b; 2003) and Glaeser et al. (1996), possess structures mathematically equivalent to certain complex systems. More important for the purposes of this article, empirical work on social interactions has focused on the analysis of precisely the type of interdependences between individual actors that lie at the heart of the microstructure of complexity-based models. My overall assessment of the empirical complexity literature is critical. The lit- erature has succeeded in describing interesting historical episodes and performing original statistical calculations that are consistent with complex systems models as well as presenting a body of regression evidence that suggests the presence of the sorts of interdependences across individuals that are a hallmark of complexity. However, this evidence is far from decisive and is amenable to alternative inter- pretations. It is therefore unclear whether this work has provided evidence in support of economic complexity per se. Following Durlauf (2001), four properties seem particularly relevant to social science contexts Nonergodicity. A system is nonergodic if the conditional probability state- ments that describe the system do not uniquely characterise the average or long-run behaviour of the system. A standard example of a nonergodic system is one where a shock at one point in time affects the long-run state of the system. Phase transition. A system exhibits a phase transition if it can undergo a qualitative change in its aggregate properties for a small change in its parameters. Phase transitions are commonplace in physical contexts. Water experiences a phase transition when its temperature moves below 0 degrees centigrade. Similarly, if one heats a magnetised piece of iron, there is a temperature above which magnetisation disappears. Emergent properties. Following ideas well described in Anderson (1972) and Crutchfield (1994), emergent properties are properties of a system that exist at a higher level of aggregation than the original description of a system. By this definition, ice is an emergent property of water. While the property of being ice describes how water molecules are collectively aligned, not of one molecule in isolation, the properties by which one molecule aligns with its neighbours are described at the level of the molecule. Similarly, magnetisation is an emergent property as it derives from the alignment of spins of individual atoms in a common piece of iron. Universality. A property is universal if its presence is robust to alternative specifications of the microstructure of the system. In physics, magnetisation is universal in the sense that its presence in iron occurs for a range of different specifications of the interdependence of spins between individual atoms. Power Laws and Scaling Laws A second area of empirical work on economic complexity has attempted to identify the presence in economic data of certain statistical properties that are associated with complex systems. In particular, this work has attempted to identify power and scaling laws. Recent research has focused on the identification of Zipf-type properties in a range of socioeconomic data. Important examples include Axtell (2001) on firm sizes and Gabaix (1999) on city sizes. Durlauf (2004) Durlauf 2004 Complexity and Empirical Economics "],["complexity-economics.html", "19 Complexity Economics", " 19 Complexity Economics The discovery that higher order phenomena cannot be directly extrapolated from lower order systems is a commonplace conclusion in genuine sciences today: it’s known as the “emergence” issue in complex systems (Nicolis and Prigogine, 1971, Ramos-Martin, 2003). The dominant characteristics of a complex system come from the interactions between its entities, rather than from the properties of a single entity considered in isolation. The fallacy in the belief that higher level phenomena (like macroeconomics) had to be, or even could be, derived from lower level phenomena (like microeconomics) was pointed out clearly in 1972—again, before Lucas wrote—by the Physics Nobel Laureate Philip Anderson: The main fallacy in this kind of thinking is that the reductionist hypothesis does not by any means imply a “constructionist” one: The ability to reduce everything to simple fundamental laws does not imply the ability to start from those laws and reconstruct the universe. (Anderson, 1972, p. 393) The impossibility of taking a “constructionist” approach to macroeconomics, as Anderson described it, means that if we are to derive a decent macroeconomics, we have to start at the level of the macroeconomy itself. This is the approach of complex systems theorists: to work from the structure of the system they are analysing, since this structure, properly laid out, will contain the interactions between the system’s entities that give it its dominant characteristics. Neoclassical macroeconomists have tried to derive macroeconomics from the wrong end—that of the individual rather than the economy—and have done so in a way that glossed over the aggregation problems that entails by pretending that an isolated individual can be scaled up to the aggregate level. It is certainly sounder—and may well be easier—to proceed in the reverse direction, by starting from aggregate statements that are true by definition, and then by disaggregating those when more detail is required. Using these definitions, it is possible to develop, from first principles that no macroeconomist can dispute, a model that does four things that no DSGE model can do: it generates endogenous cycles; it reproduces the tendency to crisis that Minsky argued was endemic to capitalism; it explains the growth of inequality over the last 50 years; and it implies that the crisis will be preceded, as it indeed was, by a “Great Moderation” in employment and inflation. The three core definitions from which a rudimentary macro-founded macroeconomic model can be derived are the employment rate (the ratio of those with a job to total population, as an indicator of both the level of economic activity and the bargaining power of workers), the wages share of output (the ratio of wages to GDP, as an indicator of the distribution of income), and, as Minsky insisted, the private debt to GDP ratio. A simple model can explain most of the behaviour of a complex system, because most of its complexity come from the fact that its components interact—and not from the well-specified behaviour of the individual components themselves So the simplest possible relationships may still reveal the core properties of the dynamic system—which in this case is the economy itself. Even at this simple level, its behaviour is far more complex than even the most advanced DSGE model, for at least three reasons. Firstly, the relationships between variables in this model aren’t constrained to be simply additive, as they are in the vast majority of DSGE models: changes in one variable can therefore compound changes in another, leading to changes in trends that a linear DSGE model cannot capture. Secondly, non-equilibrium behaviour isn’t ruled out by assumption, as in DSGE models: the entire range of outcomes that can happen is considered, and not just those that are either compatible with or lead towards equilibrium. Thirdly, the finance sector, which is ignored in DSGE models (or at best treated merely as a source of “frictions” that slow down the convergence to equilibrium), is included in a simple but fundamental way in this model, by the empirically confirmed assumption that investment in excess of profits is debt-financed With a higher propensity to invest comes the debt-driven crisis that Minsky predicted, and which we experienced in 2008. However, something that Minsky did not predict, but which did happen in the real world, also occurs in this model: the crisis is preceded by a period of apparent economic tranquillity that superficially looks the same as the transition to equilibrium in the good outcome. Before the crisis begins, there is a period of diminishing volatility in unemployment. The difference between the good and bad outcomes is the factor Minsky insisted was crucial to understanding capitalism, but which is absent from mainstream DSGE models: the level of private debt. It stabilizes at a low level in the good outcome, but reaches a high level and does not stabilize in the bad outcome. The model produces another prediction which has also become an empirical given: rising inequality. Workers’ share of GDP falls as the debt ratio rises, even though in this simple model, workers do no borrowing at all. If the debt ratio stabilises, then inequality stabilises too, as income shares reach positive equilibrium values. But if the debt ratio continues rising—as it does with a higher propensity to invest—then inequality keeps rising as well. Rising inequality is therefore not merely a “bad thing” in this model: it is also a prelude to a crisis. The dynamics of rising inequality are more obvious in the next stage in the model’s development, which introduces prices and variable nominal interest rates. As debt rises over a number of cycles, a rising share going to bankers is offset by a smaller share going to workers, so that the capitalists share fluctuates but remains relatively constant over time. However, as wages and inflation are driven down, the compounding of debt ultimately overwhelms falling wages, and profit share collapses. Steve Keen Tverberg Complexity is anything that gives structure or organization to the overall economic system. It includes any form of government or laws. The educational system is part of complexity. International trade is part of complexity. The financial system, with its money and debt, is part of complexity. The electrical system, with all its transmission needs, is part of complexity. Roads, railroads, and pipelines are part of complexity. The internet system and cloud storage are part of complexity. Wind turbines and solar panels are only possible because of complexity and the availability of fossil fuels. Storage systems for electricity, food, and fossil fuels are all part of complexity. With all this complexity, plus the energy needed to support the complexity, the economy is structured in a very different way than it would be without fossil fuels. For example, without fossil fuels, a high percentage of workers would make a living by performing subsistence agriculture. Complexity, together with fossil fuels, allows the wide range of occupations that are available today. Tverberg (2023) When the Economy Gets Squeezed by Too Little Energy "],["causal-inference.html", "20 Causal Inference", " 20 Causal Inference Cunningham In the late 20th and early 21st centuries, causal inference, a methodological form of ex post program evaluation devoid of economic theory almost entirely, became the lingua franca of empirical work because of assistant professors assigned to teach field courses. Ordinarily we think empirical practice happened downstream from econometrics. But I believe this wasn’t the case with causal inference. Its spread happened through the labor market for applied microeconomist placements, their syllabi for field courses in public, labor and development, and in time their jobs as editors, their work as referees, and over time, a paradigm shift. The paradigm shift was largely through replacements of one kind of labor/public/health economist for another, not through the replacements of one kind of econometrician for another. These labor markets favored economists trained in applied fields like labor, public, development, and health economics – fields where causal inference was gaining traction, not just as a method but as a preferred lens to view economic phenomena. Cunningham (2023) The Unconventional Path of Causal Inference in Economics "],["about.html", "A About", " A About Dyre Haugen and Dyrehaugen is Webian for Jon Martin - self-owned Globian, Webian, Norwegian and Canarian with a background from industrial research policy, urban planning and economic development consulting on global, regional and urban scales. I am deeply concerned about the (insane) way humanity (i.e. capitalism) interfere with nature. In an effort to gain insights in how and why this happens stuff is collected from around the web and put together in a linked set of web-sites. The sites are operated as personal notebooks. However, these days things can be easily published to the benefit of others concerned with the same issues. But be aware - this is not polished for presentation or peer-reviewed for exactness. I offer you just to have a look at my ‘work-desk’ as it appears in the moment. Any comment or suggestion can be mailed to dyrehaugen@gmail.com You can follow me on twitter as @dyrehaugen. Thanks for visiting! "],["links.html", "B Links", " B Links Current Dyrehaugen Sites: rcap - On Capitalism (loc) rclm - On Climate Change (loc) recs - On Economics (loc) rfin - On Finance (loc) rngy - On Energy (loc) renv - On Environment (loc) rsts - On Statistics (loc) rurb - On Urbanization (loc) rvar - On Varia (loc) rwsd - On Wisdom (loc) Blogs: rde - Blog in English (loc) rdn - Blog in Norwegian (loc) Discontinued: jdt - Collection (Jekyll) (loc) hdt - Collection (Hugo) (loc) Not listed: (q:) dhe dhn jrw56 (z:) rcsa rpad rstart "],["news.html", "C NEWS C.1 221220 Market-based development finance in crisis C.2 210717 Carney calls for stronger Government Regulation", " C NEWS C.1 221220 Market-based development finance in crisis On December 13 Ghana reached staff-level agreement on a $3 bn IMF credit package. In addition it is seeking to negotiate a 30 percent haircut with private creditors on tens of billions in bonds. Already in September Ghana’s 2026 eurobonds plunged to a record low of 59.30 cents on the US dollar. By the end of October yields had surged to 38.6 %, up from less than 11% at the end of 2021. Meanwhile, inflation is headed to 40 percent and the cedi is the worst performing currency not just in Africa but of all currencies in the world. You could shrug and say that this is Ghana’s second IMF deal in 3 years and its 17th since independence in 1957. Plus ça change. But it is more than a national crisis. It is the latest sign that the entire model of market-based development financing is in crisis. Tooze (2022) Chartbook #181: Finance and the polycrisis (6): Africa’s debt crisis C.2 210717 Carney calls for stronger Government Regulation For the world to meet its climate goals, governments would have to force industries to follow clear rules, on everything from energy generation to construction and transport, and set carbon prices that would drive investment towards green ends and close down fossil fuels. “We need clear, credible and predictable regulation from government,” he said. “Air quality rules, building codes, that type of strong regulation is needed. You can have strong regulation for the future, then the financial market will start investing today, for that future. Because that’s what markets do, they always look forward.” Without such robust intervention from governments, markets would fail to address the crisis. Gurdian "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
